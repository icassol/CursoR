<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Clustering</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/flatly.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<div class="header">
  <img style="width: 100%;" class="center-fit" src="resources/curso-banner.png"/>
</div>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>




<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 60px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h2 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h3 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h4 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h5 {
  padding-top: 65px;
  margin-top: -65px;
}
.section h6 {
  padding-top: 65px;
  margin-top: -65px;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #ffffff;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-inverse  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Curso R</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="Mod1.html">Introducción</a>
</li>
<li>
  <a href="Mod2.html">Lenguaje R</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Estadística
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Mod3.html">Basics</a>
    </li>
    <li>
      <a href="basic_statistics_in_R.html">Distribuciones</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Gráficos
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="Mod4.html">Introducción</a>
    </li>
    <li>
      <a href="Mod4_Adv.html">Avanzado</a>
    </li>
  </ul>
</li>
<li>
  <a href="Mod7.html">Predicción</a>
</li>
<li>
  <a href="Mod8.html">Clustering</a>
</li>
<li>
  <a href="Mod5.html">Reportes</a>
</li>
<li>
  <a href="Mod6.html">Web</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Clustering</h1>

</div>

<div id="TOC">
<ul>
<li><a href="#introducción"><span class="toc-section-number">1</span> Introducción</a><ul>
<li><a href="#tipos-de-clustering"><span class="toc-section-number">1.1</span> Tipos de clustering</a></li>
<li><a href="#modos-de-calcular-la-distancia"><span class="toc-section-number">1.2</span> Modos de calcular la distancia</a></li>
<li><a href="#similitud-y-correlación"><span class="toc-section-number">1.3</span> Similitud y correlación</a></li>
<li><a href="#qué-distancia-elegir"><span class="toc-section-number">1.4</span> Qué distancia elegir</a></li>
<li><a href="#preparación-de-los-datos"><span class="toc-section-number">1.5</span> Preparación de los datos</a></li>
</ul></li>
<li><a href="#clustering-de-particionado"><span class="toc-section-number">2</span> Clustering de particionado</a><ul>
<li><a href="#k-means"><span class="toc-section-number">2.1</span> K-means</a></li>
<li><a href="#k-medoids-pam"><span class="toc-section-number">2.2</span> K-medoids (PAM)</a></li>
<li><a href="#fuzzy-clustering"><span class="toc-section-number">2.3</span> Fuzzy clustering</a></li>
<li><a href="#validación-del-clustering"><span class="toc-section-number">2.4</span> Validación del clustering</a></li>
<li><a href="#principal-component-analysis-pca"><span class="toc-section-number">2.5</span> Principal Component Analysis (PCA)</a></li>
<li><a href="#t-sne"><span class="toc-section-number">2.6</span> t-SNE</a></li>
</ul></li>
<li><a href="#clustering-jerárquico"><span class="toc-section-number">3</span> Clustering jerárquico</a><ul>
<li><a href="#dendogramas"><span class="toc-section-number">3.1</span> Dendogramas</a></li>
<li><a href="#heapmap"><span class="toc-section-number">3.2</span> Heapmap</a></li>
</ul></li>
</ul>
</div>

<!-- see http://rmarkdown.rstudio.com/ for details in formatting -->
<div id="introducción" class="section level1">
<h1><span class="header-section-number">1</span> Introducción</h1>
<p>El agrupamiento (<em>clustering</em>) de los datos es una práctica dentro de la ciencia de datos que identifica grupos que contienen algún tipo de similitud.</p>
<p>El clustering tiene el objetivo de hacer predicciones acerca del <em>grupo de pertenencia</em> del dato.</p>
<p>La similitud entre datos es mide en términos de distancia, y los distintos métodos -en general- consiste en una propuesta de distancia diferente/propia.</p>
<div id="tipos-de-clustering" class="section level2">
<h2><span class="header-section-number">1.1</span> Tipos de clustering</h2>
<ul>
<li><p><em>clustering de particionado</em>: subidivide a los datos en k grupos. Uno de los métodos más conocidos es k-means, por ejemplo.</p></li>
<li><p><em>clustering jerárquico</em>: identifica grupos sin subidividir los datos.</p></li>
</ul>
</div>
<div id="modos-de-calcular-la-distancia" class="section level2">
<h2><span class="header-section-number">1.2</span> Modos de calcular la distancia</h2>
<ul>
<li>Distancia euclidea:</li>
</ul>
<p>La distancia euclidea es una medida de distancia entre dos puntos o vectores en un espacio bidimensional o multidimensional (euclidiano) basado en el teorema de Pitágoras:</p>
<p><span class="math display">\[
d_{euc}(x,y)=\sqrt{\sum_{i=1}^n (x_i-y_i)^2}   
\]</span></p>
<ul>
<li>Distancia Manhattan:</li>
</ul>
<p>La distancia de Manhattan es un concepto que surgió con ocasión del diseño de calles de la ciudad de Manhattan. Esta distancia (a veces también llamada distancia de Taxicab) está relacionada con la distancia euclidiana, pero en lugar de calcular la trayectoria diagonal más corta (línea recta) entre dos puntos, calcula la distancia diagonal pero respectando un trazado cuadricular, escalonado:</p>
<p><span class="math display">\[
\left(\sum_{i=1}^n |x_i-y_i|^p\right)^{1/p}
\]</span></p>
</div>
<div id="similitud-y-correlación" class="section level2">
<h2><span class="header-section-number">1.3</span> Similitud y correlación</h2>
<p>Vimos que una manera de encontrar similitudes es calculando la distancia entre datos. Otra manera de saber cuán similares son dos datos es a través de la correlación entre ellos: los datos están correlados, decimos que son similares. Se pueden usar diferentes tipos de métodos de correlación, tales como:</p>
<div id="distancia-de-correlación-de-pearson" class="section level3">
<h3><span class="header-section-number">1.3.1</span> Distancia de correlación de Pearson:</h3>
<p>La correlación de Pearson mide el grado de una relación lineal entre dos dataset que siguen una distribución normal. Un valor de 1 representa una perfecta correlación positiva, -1 es perfecta correlación negativa y 0 indica ausencia de correlación:</p>
<p><span class="math display">\[
\rho = \frac{\text{cov}(X,Y)}{\sigma_x \sigma_y}
\]</span></p>
<p>Donde:</p>
<p><span class="math display">\[
\text{cov}(X,Y) = \sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})\quad \text{(covarianza)} \\
\sigma{_x}^{2} = \sum_{i=1}^{n} (x_i - \bar{x})^2\quad \text{(varianza)}
\]</span></p>
<p>Entonces:</p>
<p><span class="math display">\[
d_{cor}(x,y)=1−{\frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})\quad}
{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2\quad \sum_{i=1}^{n} (y_i - \bar{y})^2\quad}}}
\]</span></p>
</div>
<div id="distancia-de-correlación-de-spearman" class="section level3">
<h3><span class="header-section-number">1.3.2</span> Distancia de correlación de Spearman:</h3>
<p>Esta distancia mide la correlación entre el rango de <code>x</code> y el rango de <code>y</code>. Se conoce como coeficiente <span class="math inline">\(\rho\)</span> (rho). Se puede considerar que es la distancia de Pearson pero aplicado a los rangos. Es útil cuando los datos no siguen una distribución normal y que tienen una relación lineal.</p>
<p><span class="math display">\[
d_{spear}(x,y)=1−{\frac{\sum_{i=1}^{n} (x_i&#39; - \bar{x}&#39;)(y_i&#39; - \bar{y}&#39;)\quad}
{\sqrt{\sum_{i=1}^{n} (x_i&#39; - \bar{x}&#39;)^2\quad \sum_{i=1}^{n} (y_i&#39; - \bar{y}&#39;)^2\quad}}}
\]</span></p>
<p>Donde:</p>
<ul>
<li><span class="math inline">\(x_i&#39;=rank(x_i)\)</span> y <span class="math inline">\(y_i&#39;=rank(y_i)\)</span>.</li>
</ul>
<p>El estadístico <span class="math inline">\(\rho\)</span> viene dado por la expresión:</p>
<p><span class="math display">\[
\rho = 1- {\frac {6 \sum d_i^2}{n(n^2 - 1)}}
\]</span></p>
<p>donde <span class="math inline">\(d\)</span> es la distancia pareada de los rangos de las variables <span class="math inline">\(x_i\)</span> e <span class="math inline">\(y_i\)</span>. Y <span class="math inline">\(n\)</span> es el número de muestras.</p>
<p>Para muestras mayores a 20 observaciones, el resultado se puede aproximar con una distribución t Student.</p>
</div>
</div>
<div id="qué-distancia-elegir" class="section level2">
<h2><span class="header-section-number">1.4</span> Qué distancia elegir</h2>
<p>La correcta elección del método de distancia tiene una gran influencia en los resultados del clustering. De manera habitual, la medida de distancia más utilizada es la distancia euclidiana.</p>
<p>Compartimos, a continuación, algunas consideraciones que pueden ser útiles a la hora de elegir el método de similitud:</p>
<ul>
<li><p>La correlación de Pearson es el método de clustering más utilizado. También se conoce como una correlación paramétrica que depende de la distribución de los datos.</p></li>
<li><p>La correlación de Spearman es un método no paramétrico y se utiliza para analizar correlaciones de rangos.</p></li>
<li><p>La correlación de Pearson es muy sensible a los <em>outliers</em>. Cuando la muestra tiene outliers, la correlación de Spearman se comporta mejor.</p></li>
<li><p>El coeficiente de correlación de Spearman es menos sensible que el de Pearson para valores que están lejos de lo esperado.</p></li>
</ul>
</div>
<div id="preparación-de-los-datos" class="section level2">
<h2><span class="header-section-number">1.5</span> Preparación de los datos</h2>
<p>Para hacer clustering de un dataset es fundamental:</p>
<ul>
<li><p>que los datos estén completos. Si hay valores ´NA´habrá que decidir si eliminar la fila, la columna, o completas los datos (con la media, o la mediana, por ejemplo).</p></li>
<li><p>que los datos estén estandarizados. Es decir: los datos deben ser comparables entre sí. En estadística los datos están estandarizados cuando la media es 0 y el desvío estandard 1.</p></li>
</ul>
<p><strong><em>Estandarización:</em></strong> Las distancias están relacionadas con la escala de los datos. Para que las comparaciones sean válidas, los datos deben estar estandarizados (o escalados). Esta necesidad se ve clara, por ejemplo, cuando se utilizan unidades de medidas como los kg, cm, litro, etc. Una fórmula típica de estandarízación es:</p>
<p><span class="math display">\[
\frac{x_i -centro(x)}{escala(x)}
\]</span></p>
<p>Donde:</p>
<ul>
<li><p><span class="math inline">\(centro(x)\)</span> puede ser la media o la mediana de x.</p></li>
<li><p><span class="math inline">\(escala(x)\)</span> puede ser el desvío estandard de x, el rango intercuantil, entre otros.</p></li>
</ul>
<p><strong><em>Hands-on sobre preparación de los datos</em></strong></p>
<ol style="list-style-type: decimal">
<li>Carga de los datos</li>
</ol>
<p>Vamos a utilizar el dataset <code>USArrests</code> que contiene estadísticas de arrestos por cada 100,000 residentes por robo, asesinato y violación en cada uno de los 50 estados de USA en 1973. Incluye también el porcentaje de la población que vive en áreas urbanas:</p>
<pre class="r"><code>data(&quot;USArrests&quot;)  
df &lt;- USArrests    # Usaremos usaremos un nombre más corto</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Barrar datos ´NA´</li>
</ol>
<pre class="r"><code>df &lt;- na.omit(df)</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Estandarizar los datos</li>
</ol>
<p>Nosotros no queremos que los algoritmos de clustering dependan de la unidad de valores que utiliza nuestro dataset. Por ese motivo, vamos a scalar/estandarizar los datos:</p>
<pre class="r"><code>df_standardized &lt;- scale(df)
head(df, n = 3)</code></pre>
<pre><code>##         Murder Assault UrbanPop Rape
## Alabama   13.2     236       58 21.2
## Alaska    10.0     263       48 44.5
## Arizona    8.1     294       80 31.0</code></pre>
<pre class="r"><code>head(df_standardized, n = 3)</code></pre>
<pre><code>##             Murder   Assault   UrbanPop         Rape
## Alabama 1.24256408 0.7828393 -0.5209066 -0.003416473
## Alaska  0.50786248 1.1068225 -1.2117642  2.484202941
## Arizona 0.07163341 1.4788032  0.9989801  1.042878388</code></pre>
<p><em>Nota:</em> Si vemos los tres primeros datos de cada matriz (de df y de df_standarized) veremos que las proporciones entre los datos de una misma columna son las mismas.</p>
<p>La función <code>scale()</code> siempre estandariza entre filas de una misma columna.</p>
<ol start="4" style="list-style-type: decimal">
<li>Instalación de paquetes</li>
</ol>
<pre class="r"><code>if (!&quot;cluster&quot; %in% rownames(installed.packages()))
  install.packages(&quot;cluster&quot;)

if (!&quot;factoextra&quot; %in% rownames(installed.packages()))
  install.packages(&quot;factoextra&quot;)

library(ggplot2)
library(cluster)
library(factoextra)</code></pre>
<pre><code>## Welcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa</code></pre>
<ol start="2" style="list-style-type: decimal">
<li>Cálculo de distancia</li>
</ol>
<p>Vamos a utilizar la distancia euclidea a nuestros datos:</p>
<pre class="r"><code>dist.eucl &lt;- dist(df_standardized, method = &quot;euclidean&quot;)
dist.eucl.matrix&lt;-as.matrix(dist.eucl) # esta matriz contiene todas las distancias
dist.eucl.matrix&lt;-round(dist.eucl.matrix,2) # redondeamos para facilitar el trabajo
head(dist.eucl.matrix[1:3,1:8])</code></pre>
<pre><code>##         Alabama Alaska Arizona Arkansas California Colorado Connecticut Delaware
## Alabama    0.00    2.7    2.29     1.29       3.26     2.65        3.22     2.02
## Alaska     2.70    0.0    2.70     2.83       3.01     2.33        4.74     3.62
## Arizona    2.29    2.7    0.00     2.72       1.31     1.37        3.26     1.91</code></pre>
<ol start="3" style="list-style-type: decimal">
<li>Visualización de las distancias</li>
</ol>
<pre class="r"><code>fviz_dist(dist.eucl, gradient = list(low = &quot;#00AFBB&quot;, mid = &quot;white&quot;, high = &quot;#FC4E07&quot;))</code></pre>
<p><img src="Mod8_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<ol start="4" style="list-style-type: decimal">
<li>Cálculo de correlación</li>
</ol>
<p>También podemos calcular la matriz de correlación de los datos. Nosotros vamos a utilizar el método de Pearson:</p>
<pre class="r"><code># Compute
library(&quot;factoextra&quot;)
dist.cor &lt;- get_dist(df_standardized, method = &quot;pearson&quot;)

# Display a subset
dist.cor.matrix&lt;-round(as.matrix(dist.cor), 2)
head(dist.cor.matrix[1:3,1:8])</code></pre>
<pre><code>##         Alabama Alaska Arizona Arkansas California Colorado Connecticut Delaware
## Alabama    0.00   0.71    1.45     0.09       1.87     1.69        1.71     1.14
## Alaska     0.71   0.00    0.83     0.37       0.81     0.52        1.86     1.48
## Arizona    1.45   0.83    0.00     1.18       0.29     0.60        0.78     0.34</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Visualización de las correlaciones:</li>
</ol>
<pre class="r"><code>fviz_dist(dist.cor)</code></pre>
<p><img src="Mod8_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
</div>
</div>
<div id="clustering-de-particionado" class="section level1">
<h1><span class="header-section-number">2</span> Clustering de particionado</h1>
<div id="k-means" class="section level2">
<h2><span class="header-section-number">2.1</span> K-means</h2>
<p><strong><em>Conceptos básicos</em></strong></p>
<p>La idea básica detrás de la agrupación de k-means consiste en definir grupos de modo que la variación de distancias dentro de cada grupo (conocida como <em>total within-cluster variation</em>) sea mínima.</p>
<p>Hay varios algoritmos de k-means disponibles. El algoritmo estándar es el algoritmo Hartigan-Wong (Hartigan y Wong 1979), que define la variación total dentro del grupo como la suma de las distancias cuadradas de las distancias euclidianas entre los elementos y el centroide correspondiente:</p>
<p><span class="math display">\[
W(C_k)=\sum_{x_i \in C_k}^{} (x_i - \mu_k)^2
\]</span></p>
<p>Donde:</p>
<ul>
<li><p><span class="math inline">\(x_i\)</span> es un punto que pertenece al cluster <span class="math inline">\(C_k\)</span>.</p></li>
<li><p><span class="math inline">\(\mu_k\)</span> es el valor medio de los puntos del cluster <span class="math inline">\(C_k\)</span>.</p></li>
</ul>
<p>Cada observación (<span class="math inline">\(x_i\)</span>) se asigna a un grupo dado de tal manera que la distancia de la suma de los cuadrados de la observación a sus centros de grupo asignados <span class="math inline">\(\mu_k\)</span> es un mínimo.</p>
<p>Definimos la variación total dentro del cluster de la siguiente manera:</p>
<p><span class="math display">\[
total within-cluster=\sum_{k=1}^{k}W(C_k)=\sum_{k=1}^{k}\sum_{x_i \in C_k}^{} (x_i - \mu_k)^2
\]</span></p>
<p>El algoritmo de K-means se puede resumir en los siguientes pasos:</p>
<ol style="list-style-type: decimal">
<li><p>Se especifica el número de grupos (K) que se crearán (dato señalado por el analista).</p></li>
<li><p>Seleccionar aleatoriamente k objetos del conjunto de datos como los centros o medios iniciales del grupo. A estos objetos se los llamada <em>centroides</em>.</p></li>
<li><p>Asignar cada observación a su centroide más cercano, basándose en la distancia euclidiana entre el objeto y el centroide.</p></li>
<li><p>Para cada uno de los k clusters, actualizar el centroide del cluster calculando los nuevos valores medios de todos los puntos de datos en el cluster.</p></li>
<li><p>Minimizar iterativamente el total dentro de la suma de cuadrados. Es decir, repetir los pasos 3 y 4 hasta que las asignaciones de clúster dejen de cambiar o se alcance el número máximo de iteraciones. De forma predeterminada, R utiliza 10 como valor predeterminado para el número máximo de iteraciones.</p></li>
</ol>
<p><strong><em>Estimación del k óptimo</em></strong></p>
<p>Se puede hallar el óptimo número K para los datos a través de la función <code>fviz_nbclust()</code>. Veamos un ejemplo con el mismo set de datos:</p>
<ol style="list-style-type: decimal">
<li>Obtener K óptimo</li>
</ol>
<pre class="r"><code>library(cluster)
library(factoextra)
# recordemos que df_standardized es el dataset estandarizado
factoextra::fviz_nbclust(df_standardized, kmeans, method = &quot;gap_stat&quot;)</code></pre>
<p><img src="Mod8_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>El número óptimo de grupo que sugiere <code>fviz_nbclust()</code> es 3. Es decir, K=3.</p>
<ol start="2" style="list-style-type: decimal">
<li>Calcular los grupos y visualizarlos</li>
</ol>
<pre class="r"><code>set.seed(123) # setear la semilla para que siempre los resultados sean iguales
km.res &lt;- kmeans(df_standardized, 3, nstart = 25)
# visualización
fviz_cluster(km.res, data = df_standardized, palette = &quot;jco&quot;,
             ggtheme = theme_minimal(), star.plot = TRUE, repel = TRUE)</code></pre>
<p><img src="Mod8_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p><em>Nota:</em> en <code>kmeans()</code> hemos utilizado el parámetro <code>nstart = 25</code>. Esto significa que R calculará 25 veces variación de distancia dentro de cada grupo, y se quedará con el menor valor. El valor predeterminado de <code>nstart</code> en R es uno. Sin embargo, se recomienda encarecidamente calcular k-means clustering con un gran valor de nstart como 25 o 50, para obtener un resultado más estable.</p>
<blockquote>
<p>Observe, en el gráfico, que cada grupo tiene señalado sus centrómero (un triángulo, un círculo y un cuadrado más grande que el resto).</p>
</blockquote>
<ol start="3" style="list-style-type: decimal">
<li>Veamos la información de <code>km.res</code>:</li>
</ol>
<pre class="r"><code>print(km.res)</code></pre>
<pre><code>## K-means clustering with 3 clusters of sizes 20, 13, 17
## 
## Cluster means:
##       Murder    Assault   UrbanPop       Rape
## 1  1.0049340  1.0138274  0.1975853  0.8469650
## 2 -0.9615407 -1.1066010 -0.9301069 -0.9667633
## 3 -0.4469795 -0.3465138  0.4788049 -0.2571398
## 
## Clustering vector:
##        Alabama         Alaska        Arizona       Arkansas     California       Colorado 
##              1              1              1              3              1              1 
##    Connecticut       Delaware        Florida        Georgia         Hawaii          Idaho 
##              3              3              1              1              3              2 
##       Illinois        Indiana           Iowa         Kansas       Kentucky      Louisiana 
##              1              3              2              3              2              1 
##          Maine       Maryland  Massachusetts       Michigan      Minnesota    Mississippi 
##              2              1              3              1              2              1 
##       Missouri        Montana       Nebraska         Nevada  New Hampshire     New Jersey 
##              1              2              2              1              2              3 
##     New Mexico       New York North Carolina   North Dakota           Ohio       Oklahoma 
##              1              1              1              2              3              3 
##         Oregon   Pennsylvania   Rhode Island South Carolina   South Dakota      Tennessee 
##              3              3              3              1              2              1 
##          Texas           Utah        Vermont       Virginia     Washington  West Virginia 
##              1              3              2              3              3              2 
##      Wisconsin        Wyoming 
##              2              3 
## 
## Within cluster sum of squares by cluster:
## [1] 46.74796 11.95246 19.62285
##  (between_SS / total_SS =  60.0 %)
## 
## Available components:
## 
## [1] &quot;cluster&quot;      &quot;centers&quot;      &quot;totss&quot;        &quot;withinss&quot;     &quot;tot.withinss&quot; &quot;betweenss&quot;   
## [7] &quot;size&quot;         &quot;iter&quot;         &quot;ifault&quot;</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>Quedarnos, por ejemplo, con los datos del cluster 2:</li>
</ol>
<pre class="r"><code>km.res$cluster[km.res$cluster==2]</code></pre>
<pre><code>##         Idaho          Iowa      Kentucky         Maine     Minnesota       Montana      Nebraska 
##             2             2             2             2             2             2             2 
## New Hampshire  North Dakota  South Dakota       Vermont West Virginia     Wisconsin 
##             2             2             2             2             2             2</code></pre>
<ol start="4" style="list-style-type: decimal">
<li>También podemos ver las medias de cada grupo:</li>
</ol>
<pre class="r"><code>aggregate(df, by=list(cluster=km.res$cluster), mean)</code></pre>
<pre><code>##   cluster    Murder   Assault UrbanPop     Rape
## 1       1 12.165000 255.25000 68.40000 29.16500
## 2       2  3.600000  78.53846 52.07692 12.17692
## 3       3  5.841176 141.88235 72.47059 18.82353</code></pre>
<p>Ejemplo de interprestación de la matriz anterior: <em>Los habitantes del grupo 1 tienen una media de asesinatos (</em>Murder<em>) de 3.6, mientras que los habitantes del grupo 2 tiene una media de 5.83 y los de grupos 3 una media de 12.16 asesinatos por 1000 habitantes.</em></p>
<ol start="5" style="list-style-type: decimal">
<li>Podemos probar con <code>cluster=4</code>:</li>
</ol>
<pre class="r"><code>set.seed(123) # setear la semilla para que siempre los resultados sean iguales
km.res &lt;- kmeans(df_standardized, 4, nstart = 25)
# visualización
fviz_cluster(km.res, data = df_standardized, palette = &quot;jco&quot;,
             ggtheme = theme_minimal(), star.plot = TRUE, repel = TRUE)</code></pre>
<p><img src="Mod8_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<blockquote>
<p>La visualización de los grupos es una herramienta para el ajuste del número de grupos. La función <code>fviz_nbclust</code> es una buena herramienta pero puede suceder que, según el criterio del analista, convenga otro número para k.</p>
</blockquote>
<p><strong><em>Ventajas y desventajas</em></strong></p>
<p>K-means es uno de los métodos de clustering más utilizados. Destaca por la sencillez y velocidad de su algoritmo, sin embargo, presenta una serie de limitaciones que se deben tener en cuenta.</p>
<ul>
<li><p>Requiere que se indique de antemano el número de clusters que se van a crear. Esto puede ser complicado si no se dispone de información adicional sobre los datos con los que se trabaja.</p></li>
<li><p>Las agrupaciones resultantes pueden variar dependiendo de la asignación aleatoria inicial de los centroides. Para minimizar este problema se recomienda repetir el proceso de clustering entre 25-50 veces y seleccionar como resultado definitivo el que tenga menor suma total de varianza interna.</p></li>
<li><p>Presenta problemas de robustez frente a outliers. La única solución es excluirlos o recurrir a otros métodos de clustering más robustos como K-medoids (PAM).</p></li>
</ul>
</div>
<div id="k-medoids-pam" class="section level2">
<h2><span class="header-section-number">2.2</span> K-medoids (PAM)</h2>
<p>K-medoids es un método de clustering muy similar a K-means en cuanto a que ambos agrupan las observaciones en K clusters, donde K es un valor preestablecido por el analista. La diferencia es que, en K-medoids, cada cluster está representado por una observación presente en el cluster (medoid), mientras que en K-means cada cluster está representado por su centroide, que se corresponde con el promedio de todas las observaciones del cluster pero con ninguna en particular.</p>
<p><strong><em>medoid:</em></strong> elemento dentro de un cluster cuya distancia (diferencia) promedio entre él y todos los demás elementos del mismo cluster es lo menor posible. Este elemento se puede considerar como el más representativo del grupo.</p>
<p>El algoritmo más empleado para aplicar K-medoids se conoce como PAM (Partitioning Around Medoids).</p>
<div id="hands-on" class="section level3">
<h3><span class="header-section-number">2.2.1</span> Hands-on</h3>
<pre class="r"><code>pam.res &lt;- pam(df_standardized, 4, metric = &quot;manhattan&quot;)
fviz_cluster(pam.res, geom = c(&quot;point&quot;, &quot;text&quot;), ellipse.type = &quot;t&quot;, ggtheme = theme_minimal(), show.clust.cent = TRUE, labelsize = 8, star.plot = TRUE, repel = TRUE)</code></pre>
<p><img src="Mod8_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
</div>
<div id="fuzzy-clustering" class="section level2">
<h2><span class="header-section-number">2.3</span> Fuzzy clustering</h2>
<p>Los métodos de clustering descritos hasta ahora (K-means, K-medoids) asignan cada observación únicamente a un cluster, de ahí que también se conozcan como hard clustering. Los métodos de fuzzy clustering o soft clustering se caracterizan porque, cada observación, puede pertenecer potencialmente a varios clusters, en concreto, cada observación tiene asignado un grado de pertenencia a cada uno de los cluster. Fuzzy c-means (FCM) es uno de los algoritmos más empleado para generar fuzzy clustering.</p>
<p><strong><em>Aplicación de FCM a los datos de USArrests:</em></strong></p>
<pre class="r"><code>library(cluster)
fuzzy_cluster &lt;- fanny(x = df_standardized, diss = FALSE, k = 3, metric = &quot;euclidean&quot;,
                       stand = FALSE)
head(fuzzy_cluster$membership)</code></pre>
<pre><code>##                 [,1]      [,2]      [,3]
## Alabama    0.4676004 0.3144516 0.2179480
## Alaska     0.4278809 0.3178707 0.2542484
## Arizona    0.5092197 0.2945668 0.1962135
## Arkansas   0.2934077 0.3787718 0.3278205
## California 0.4668527 0.3084149 0.2247324
## Colorado   0.4542018 0.3236683 0.2221299</code></pre>
<p>Esto significa que el estado de Alabama tiene un 46 % de ser asociado al cluster 1, un 31 % al cluster 2 y un 21 % al cluster 3.</p>
<p>Visualización de los datos:</p>
<pre class="r"><code>library(factoextra)
fviz_cluster(object = fuzzy_cluster, repel = TRUE, ellipse.type = &quot;norm&quot;,
             pallete = &quot;jco&quot;) + theme_bw() + labs(title = &quot;Fuzzy Cluster plot&quot;)</code></pre>
<p><img src="Mod8_files/figure-html/unnamed-chunk-17-1.png" width="672" /></p>
</div>
<div id="validación-del-clustering" class="section level2">
<h2><span class="header-section-number">2.4</span> Validación del clustering</h2>
<p>Los métodos de clustering tienen la propiedad de encontrar agrupaciones en cualquier set de datos, independientemente de que realmente existan o no dichos grupos. La validación de clusters es el proceso por el cual se evalúa la veracidad de los grupos obtenidos. Para facilitar la validación, explicaremos brevemente dos aspectos a tener en cuenta: estudio de la tendencia de clustering y estudio de la calidad.</p>
<div id="estudio-de-la-tendencia-de-clustering" class="section level3">
<h3><span class="header-section-number">2.4.1</span> Estudio de la tendencia de clustering</h3>
<p>Antes de aplicar un método de clustering a los datos es conveniente evaluar si hay indicios de que realmente existe algún tipo de agrupación en ellos. A este proceso se le conoce como assessing cluster tendecy y puede llevarse a cabo mediante test estadísticos (<em>Estadístico de Hopkins</em>) o de forma visual (<em>Visual Assessment of cluster Tendency</em>).</p>
<p>Para ilustrar la importancia de este pre-análisis inicial, se aplica clustering a dos sets de datos, uno que sí contiene grupos reales (<code>iris</code>) y otro aleatoriamente simulado que no.</p>
<pre class="r"><code>library(purrr)

# Se elimina la columna que contiene la especie de planta
datos_iris &lt;- iris[, -5]

# Se generan valores aleatorios dentro del rango de cada variable. Se utiliza la
# función map del paquete purrr.
datos_simulados &lt;- map_df(datos_iris,
                          .f = function(x){runif(n = length(x),
                                                 min = min(x),
                                                 max = max(x))
                                          }
                          )

# Estandarización de los datos
datos_iris      &lt;- scale(datos_iris)
datos_simulados &lt;- scale(datos_simulados)</code></pre>
<p>Una representación gráfica permite comprobar que el set de datos <code>iris</code> sí contiene grupos reales, mientras que los datos simulados no. Al haber más de dos variables es necesario reducir la dimensionalidad mediante un <em>Principal Component Analysis</em> (método que explicaremos más adelante).</p>
<pre class="r"><code>library(factoextra)
library(ggpubr)
pca_datos_iris      &lt;- prcomp(datos_iris)
pca_datos_simulados &lt;- prcomp(datos_simulados)
p1 &lt;- fviz_pca_ind(X = pca_datos_iris, habillage = iris$Species,
                   geom = &quot;point&quot;, title = &quot;PCA - datos iris&quot;,
                   pallete = &quot;jco&quot;) +
      theme_bw() + theme(legend.position = &quot;bottom&quot;)
p2 &lt;- fviz_pca_ind(X = pca_datos_simulados, geom = &quot;point&quot;,
                   title = &quot;PCA - datos simulados&quot;, pallete = &quot;jco&quot;) +
      theme_bw() + theme(legend.position = &quot;bottom&quot;)

ggarrange(p1, p2, common.legend = TRUE)</code></pre>
<p><img src="Mod8_files/figure-html/unnamed-chunk-19-1.png" width="672" /></p>
<p>Véase que ocurre cuando se aplican métodos de clustering a estos dos sets de datos.</p>
<pre class="r"><code># K-means clustering
km_datos_iris &lt;- kmeans(x = datos_iris, centers = 3)
p1 &lt;- fviz_cluster(object = km_datos_iris, data = datos_iris,
                   ellipse.type = &quot;norm&quot;, geom = &quot;point&quot;, main = &quot;Datos iris&quot;,
                   stand = FALSE, palette = &quot;jco&quot;) +
      theme_bw() + theme(legend.position = &quot;none&quot;)
km_datos_simulados &lt;- kmeans(x = datos_simulados, centers = 3)
p2 &lt;- fviz_cluster(object = km_datos_simulados, data = datos_simulados,
                   ellipse.type = &quot;norm&quot;, geom = &quot;point&quot;,
                   main = &quot;Datos simulados&quot;, stand = FALSE, palette = &quot;jco&quot;) +
      theme_bw() + theme(legend.position = &quot;none&quot;)

# Hierarchical clustering
p3 &lt;- fviz_dend(x = hclust(dist(datos_iris)), k = 3, k_colors = &quot;jco&quot;,
                show_labels = FALSE, main = &quot;Datos iris&quot;)
p4 &lt;- fviz_dend(x = hclust(dist(datos_simulados)), k = 3, k_colors = &quot;jco&quot;,
                show_labels = FALSE, main = &quot;Datos simulados&quot;)

ggarrange(p1, p2)</code></pre>
<p><img src="Mod8_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<pre class="r"><code>ggarrange(p3, p4)</code></pre>
<p><img src="Mod8_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>Ambos métodos de clustering crean agrupaciones en el set de datos simulado. De no analizarse con detenimiento, podrían darse por válidos estos grupos aun cuando realmente no existen. A continuación, se muestran dos métodos que ayudan a identificar casos como este y prevenir la utilización de clustering en escenarios en los que no tiene sentido hacerlo.</p>
<p><strong><em>Estadístico de Hopkins</em></strong></p>
<p>El estadístico Hopkins permite evaluar la tendencia de clustering de un conjunto de datos mediante el cálculo de la probabilidad de que dichos datos procedan de una distribución uniforme, es decir, estudia la distribución espacial aleatoria de las observaciones.</p>
<pre class="r"><code># datos_iris &lt;- iris[, -5]
# datos_simulados &lt;- map_df(datos_iris,
#                           .f = function(x){runif(n = length(x), min = min(x), max = max(x))})
if (!&quot;clustertend&quot; %in% rownames(installed.packages()))
  install.packages(&quot;clustertend&quot;)
library(clustertend)
set.seed(321)

# Estadístico H para el set de datos iris
hopkins(data = datos_iris, n = nrow(datos_iris) - 1)</code></pre>
<pre><code>## $H
## [1] 0.1842089</code></pre>
<pre class="r"><code># Estadístico H para el set de datos simulado
hopkins(data = datos_simulados, n = nrow(datos_simulados) - 1)</code></pre>
<pre><code>## $H
## [1] 0.5198121</code></pre>
<p>El set de datos <code>iris</code> no sigue una distribución espacial uniforme, su estructura contiene algún tipo de agrupación. Por contra, el valor del estadístico H obtenido para el set de datos simulado es muy próximo a 0.5, lo que indica que los datos están uniformemente distribuidos y desaconseja la utilización de métodos de clustering.</p>
</div>
<div id="estudio-de-la-calidad" class="section level3">
<h3><span class="header-section-number">2.4.2</span> Estudio de la calidad</h3>
<p>La calidad de los clusters se puede analizar desde tres tipos de estadísticos:</p>
<ul>
<li><p>Validación interna de los clusters: Emplean únicamente información interna del proceso de clustering. Se trata de un proceso totalmente <em>unsupervised</em>.</p></li>
<li><p>Validación externa de los clusters (<em>ground truth</em>): Combinan los resultados del clustering (<em>unsupervised</em>) con información externa (<em>supervised</em>), como puede ser un set de validación en el que se conoce el verdadero grupo al que pertenece cada observación.</p></li>
<li><p>Significancia de los clusters: Calculan la probabilidad (<em>p-value</em>) de que los clusters generados se deban únicamente al azar.</p></li>
</ul>
<p><strong><em>Validación interna:</em></strong></p>
<p>El clustering agrupa observaciones de forma que sean similares a aquellas que están dentro de un mismo cluster, es decir, que la homogeneidad (también llamada compactness o cohesion) se lo mayor posible a la vez que lo es la separación entre clusters. Cuantificar estas dos características es una forma de evaluar cómo de bueno es el resultado obtenido.</p>
<p>Dado que la <em>homogeneidad</em> y la <em>separación</em> siguen tendencias opuestas (a mayor número de clusters la homogeneidad aumenta, pero la separación disminuye), algunos índices combinan ambas medidas, dos de ellos son: el <em>silhouette width</em> y el índice <em>Dunn</em>.</p>
<p><em>Silhouette width:</em> Cuantifica cómo de buena es la asignación que se ha hecho de una observación comparando su similitud con el resto de observaciones del mismo cluster frente a las de los otros clusters. Su valor puede estar entre -1 y 1, siendo valores altos un indicativo de que la observación se ha asignado al cluster correcto. Cuando su valor es próximo a cero significa que la observación se encuentra en un punto intermedio entre dos clusters. El uso combinado de las funciones eclust() y fviz_silhouette() del paquete factoextra() permiten obtener los coeficientes silhouette de forma sencilla:</p>
<pre class="r"><code>library(factoextra)
# Se emplean los datos iris excluyendo la variable Species
datos &lt;- scale(iris[, -5])
km_clusters &lt;- eclust(x = datos, FUNcluster = &quot;kmeans&quot;, k = 3, seed = 123,
                      hc_metric = &quot;euclidean&quot;, nstart = 50, graph = FALSE)
fviz_silhouette(sil.obj = km_clusters, print.summary = TRUE, palette = &quot;jco&quot;,
                ggtheme = theme_classic())</code></pre>
<pre><code>##   cluster size ave.sil.width
## 1       1   50          0.64
## 2       2   53          0.39
## 3       3   47          0.35</code></pre>
<p><img src="Mod8_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>La función eclust() almacena, además de la información devuelta por la función de clustering empleada, en este caso kmeans, información sobre los coeficientes silhouette individuales y por cluster, el cluster al que se ha asignado cada observación y el cluster vecino más próximo (el segundo mejor candidato).</p>
<pre class="r"><code># Media silhouette por cluster
km_clusters$silinfo$clus.avg.widths</code></pre>
<pre><code>## [1] 0.6363162 0.3933772 0.3473922</code></pre>
<pre class="r"><code># Coeficiente silhouette para cada observación
head(km_clusters$silinfo$widths)</code></pre>
<pre><code>##    cluster neighbor sil_width
## 1        1        2 0.7341949
## 41       1        2 0.7333345
## 8        1        2 0.7308169
## 18       1        2 0.7287522
## 5        1        2 0.7284741
## 40       1        2 0.7247047</code></pre>
<p>El cluster número 2 (amarillo) tiene observaciones con valores de silhouette próximos a 0 e incluso negativos, lo que indica que esas observaciones podrían estar mal clasificadas. Viendo la representación gráfica del clustering, cabe esperar que sean observaciones que están situadas en la frontera entre los clusters 2 y 3 ya que solapan.</p>
<p>Véase cómo cambia el resultado si en lugar de 3 clusters (número correcto de especies), se crean 5.</p>
<pre class="r"><code>library(ggpubr)
km_clusters &lt;- eclust(x = datos, FUNcluster = &quot;kmeans&quot;, k = 5, seed = 123, 
                      hc_metric = &quot;euclidean&quot;, nstart = 50, graph = FALSE)
p1 &lt;- fviz_cluster(object = km_clusters, geom = &quot;point&quot;, ellipse.type  = &quot;norm&quot;,
                   palette = &quot;jco&quot;) +
      theme_classic() + theme(legend.position = &quot;none&quot;) 

p2 &lt;- fviz_silhouette(sil.obj = km_clusters, print.summary = FALSE,
                      palette = &quot;jco&quot;, ggtheme = theme_classic()) +
      theme(legend.position = &quot;none&quot;)

ggarrange(p1, p2)</code></pre>
<p><img src="Mod8_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
</div>
</div>
<div id="principal-component-analysis-pca" class="section level2">
<h2><span class="header-section-number">2.5</span> Principal Component Analysis (PCA)</h2>
<p>El clustering de datos, en general, está basado en un cálculo de distancias. La similitud entre dos datos datos es la distancia entre dos puntos. Hemos visto que el método K-means agrupa puntos (x,y) a grupos según la distancia del mismo respecto a un centrómero. En la ciencia de datos, habitualmente los datos son multivariados. Es decir: un dato no son dos variables (x,y) sino muchas.</p>
<p>Existe un método que transforma un dato multivariado a un punto de dos variables. El criterio de transformación está basado en que ambos universo de datos debe respetar el mismo patrón de distancia entre datos. Por ejemplo: suponga que un dato tiene cuatro variables: <span class="math inline">\(X_1,X_2,X_3,X_4\)</span>, y <span class="math inline">\(f(X)\)</span> es la transformación tal que <span class="math inline">\(f(X)=(Y_1,Y_2)\)</span> donde <span class="math inline">\(Y\)</span> es el dato transformado, entonces la <span class="math inline">\(d(X_1,X_2,X_3,X_4)=d(Y_1,Y_2)\)</span>.</p>
<p>Principal Component Analysis es un método estadístico que permite simplificar la complejidad de espacios muestrales con muchas dimensiones a la vez que conserva su información. Es la función de transformación <span class="math inline">\(f(X)\)</span>. La compactación de dimensiones no debe ser, necesariamente, a dos variables. Sólo se requiere que sea menor a la dimensión original.</p>
<p>Es un método que se suele utilizar previo al clustering a la regresión.</p>
<p>A pesar de las muy buenas propiedades que tiene el PCA, sufre de algunas limitaciones, por ejemplo, solo tiene en cuenta combinaciones lineales de las variables originales. En determinados escenarios, el no poder considerar otro tipo de combinaciones supone perder mucha información.</p>
<p>Para conocer el funcionamiento del algoritmos de PCA recomendamos leer <a href="https://www.cienciadedatos.net/documentos/35_principal_component_analysis">este hands-on</a>.</p>
</div>
<div id="t-sne" class="section level2">
<h2><span class="header-section-number">2.6</span> t-SNE</h2>
</div>
</div>
<div id="clustering-jerárquico" class="section level1">
<h1><span class="header-section-number">3</span> Clustering jerárquico</h1>
<p>El resultado de este tipo de clustering es un árbol.</p>
<div id="dendogramas" class="section level2">
<h2><span class="header-section-number">3.1</span> Dendogramas</h2>
<pre class="r"><code>res.hc &lt;- hclust(dist.eucl,  method = &quot;ward.D2&quot;)
fviz_dend(res.hc, cex = 0.5, k = 4, palette = &quot;jco&quot;) </code></pre>
<p><img src="Mod8_files/figure-html/unnamed-chunk-27-1.png" width="672" /></p>
<div id="particionamiento-del-dendograma" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Particionamiento del dendograma</h3>
<p>Es posible establecer grupos y manejarlos de manera independiente. Por ejemplo:</p>
<pre class="r"><code>clus4 = cutree(res.hc, 4) # establecemos 4 clusters
which(clus4==1) # nos quedamos con los estados del 1º cluster</code></pre>
<pre><code>##        Alabama        Georgia      Louisiana    Mississippi North Carolina South Carolina 
##              1             10             18             24             33             40 
##      Tennessee 
##             42</code></pre>
<p>Se puede visualizar el corte en el dendograma con una línea:</p>
<pre class="r"><code>plot(res.hc, hang = -1, cex = 0.6)
abline(h = 4, lty = 2) # dibujar la línea en la altura 4. </code></pre>
<p><img src="Mod8_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<blockquote>
<p>Hemos comentado que el método K-means tiene la desventaja que requiere la definición del número de clusters. Esta limitación se puede contrarrestar combinandolo con un dendrograma: 1º graficar un dendograma, 2º cortar el árbol a la altura que, visualmente se considere conveniente, 3º utilizar esa altura como K para el método k-means.</p>
</blockquote>
</div>
</div>
<div id="heapmap" class="section level2">
<h2><span class="header-section-number">3.2</span> Heapmap</h2>
<p>Los mapas de calor son otro modo muy completo de ver agrupamientos jerárquicos.</p>
<p>En los mapas de calor, generalmente, las columnas son muestras y las filas son variables. Por lo tanto, comenzamos por transponer los datos antes de crear el mapa de calor.</p>
<pre class="r"><code>if (!&quot;pheatmap&quot; %in% rownames(installed.packages()))
  install.packages(&quot;pheatmap&quot;)

library(pheatmap)
pheatmap(t(df_standardized), cutree_cols = 4)</code></pre>
<p><img src="Mod8_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<p>Bibliografía recomendada:</p>
<ul>
<li><a href="https://rpubs.com/Joaquin_AR/310338">Clustering y heatmaps: aprendizaje no supervisado</a></li>
</ul>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
