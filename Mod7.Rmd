---
title: 'Regresión'
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_depth: 2
---

<!-- see http://rmarkdown.rstudio.com/ for details in formatting -->
```{r style, echo = FALSE, results='hide', message=FALSE, warning=FALSE}
if (!"BiocStyle" %in% rownames(installed.packages()))
  BiocManager::install("BiocStyle")
library(BiocStyle)
BiocStyle::markdown()
options(width=100)
knitr::opts_chunk$set(cache=T, autodep=TRUE)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Regresiones lineales 

Teniendo un conjunto de datos, y suponiendo que cada dato es un par ordenado (x,y), una ***regresión lineal simple*** genera una recta cuyos puntos (x',y') tienen la mínima distancia posible respecto a (x,y). 

El modelo anterior implica que la regresión lineal simple considera a uno de los ejes de coordenadas como variable dependiente y al otro como variable independiente. 

Veamos un ejemplo y luego seguiremos explicando los conceptos: 

Trabajaremos sobre una muestra de 20 pacientes. De cada uno de ellos sabemos su presión sanguinea (BP: blood pressure) y su edad (Age). Es decir: 

  - el conjunto de datos son los 20 pacientes. 
  - el par ordenado (x,y) es (BP, Age).
  - el objetivo es generar una recta cuyos puntos (x',y') tengan la menor distancia respecto de (x,y). Al valor y' se le llama **predicción**. 

Carguemos y grafiquemos los datos: 

```{r cars, message=FALSE, warning=FALSE}
Age<-c(67, 40, 68, 64, 47, 71, 54, 59, 46, 52, 57, 63, 45, 55, 48, 51, 48, 77, 38, 61)
BP<-c(185, 154, 198, 189, 168, 200, 166, 190, 155, 172, 180, 173, 176, 172, 150, 176, 155, 192, 154, 185)

plot(Age,BP, pch =16); # seteamos puntos rellenos

```

La nube de puntos sigue una forma lineal (es un comportamiento similar a una recta). 

Contando con los arreglos `BP` y `Age` es posible saber, por ejemplo, que un paciente de 67 años, tiene una presión sanguínea de 185. O sea, la pregunta 

> *¿qué presión tendrá un paciente con una edad determinada?*

la podremos hacer 20 veces (preguntando por cada edad del arreglo `Age`).

La idea de la regresión lineal es poder generalizar la pregunta anterior a cualquier edad. Esa información, precisamente, es la que contiene la recta. De esta manera, contando con la regresión lineal podremos **predecir** qué presión sanguinea tendrá, por ejemplo, una persona de 53 años (53 es una edad que no existe en el arreglo `Age`).

La recta que "pasará cerca" de los 20 puntos tendrá como variable indepediente la edad, y como variable dependiente la presión sanguinea. 

En R es muy simple generar la regresión lineal: 

```{r}
model<-lm(BP~Age);
coef(model);
```

El formato del parámetro del comando `lm()` es `lm([columna a predecir] ~ [término/s que se utilizarán para construir el modelo lineal])`

El valor independiente de la recta será el `Intercept` y la pendiente será `Age`. Habitualmente al Intercept se le dice B0 y a la pendiente B1. 

Tarea: ¿qué información aportan los valores B0 y B1? 

Es posible conocer valores estadísticos del modelo: 

```{r}
summary(model)
```

Este resumen contiene información muy valiosa acerca de la calidad del modelo. Para este curso, por el momento, destacaremos que:

- B0 y B1 -ambos- son "muy significativos estadísticamente" ya que sus p-values (4.44e-09 y 1.44e-09) son muy pequeños. 
- El estadístico F también es muy significativo (1.439e-06).

# Residuos

Si (x', y') es un punto de la recta del modelo lineal, y (x, y) es un punto de la población, existe una diferencia entre la observación (y) y la predicción (y') llamada ***residuo***.

***Validación de la regresión***. El análisis de los residuos del modelo lineal indica si el modelo representa correctamente a la muestra. A los efectos prácticos, para que una muestra pueda ser modelada por una regresión lineal, se deben cumplir dos requisitos: 

1. La muestra no debe tener **outliers** (valores atípicos). No debe haber puntos que estén lejanos a la nube de puntos que se asemeja -en conjunto- a una recta. Este requisito lo vamos chequear visualmente: se plotean los datos y se observan los puntos. 

2. Los residuos deben ser independientes y seguir una distribución normal. Este requisito se puede chequear con esta instrucción: 

```{r}
model.stdres=rstandard(model)

qqnorm(model.stdres,
  ylab="Standardized Residuals",
  xlab="Normal Scores")

qqline(model.stdres)

```

Si los residuos siguen una distribución normal, se espera que `qqline` sea recta. En este caso el resultado es bueno.

## Test de Shapiro

Para comprobar si un conjunto de datos siguen una distribución normal, se puede usar el test de Shapiro. Este test, entonces, lo podemos aplicar al conjunto de residuos del modelo lineal de manera que sepamos si, efectivamente, el modelo es válido para representar a la muestra. Por ejemplo, para una distribución normal:

```{r}
set.seed(5)
xtest <-  rnorm(100, mean = 5, sd = 2)
shapiro.test(xtest)
```

El p-value no es significativo (0.445) y, por lo tanto, aceptamos la hipótesis nula (los datos siguen una distribución normal).

Sin embargo, si los datos no siguen una distribución normal, entonces el test de Shapiro da un p-value pequeño y, por lo tanto, rechazamos la hipótesis nula. Ejemplo: 

```{r}
set.seed(5)
xtest <-  rchisq(100, 3) # distribución Chi-Cuadrado
shapiro.test(xtest)
```

## Ejemplo del impacto de los outliers

Esta sección apunta a mostrar la importancia de identificar los outliers (valores atípicos)  para utilizar una regresión lineal como modelo de representación de la muestra (el primer requisito indicado en la sección ***Residuos***). 

En 1973, el estadístico Francis Anscombe, diseñó un ejemplo con cuatro conjuntos de datos que tienen estadísticas descriptivas simples casi idénticas, pero que parecen muy diferentes cuando se grafican. Cada conjunto de datos consta de once (x, y) puntos. Este ejemplo sirve para mostrar:
  - la importancia de graficar los datos antes de analizarlos 
  - el efecto de los valores atípicos (outliers) en las propiedades estadísticas.

```{r}
anscombe
apply(anscombe,2,var) # la varianza de cada columna
apply(anscombe,2,mean) # la media de cada columna
```

Se ven muy similares. Construyamos una regresión lineal para los 4 conjuntos: 

```{r}
coef(lm(y1~x1,data=anscombe))
coef(lm(y2~x2,data=anscombe))
coef(lm(y3~x3,data=anscombe))
coef(lm(y4~x4,data=anscombe))
```

Las cuatro rectas son idénticas. 

Generemos los cuatro modelos de manera más eficiente, guardándolos en una lista: 

```{r}
summary(anscombe)

##-- now some "magic" to do the 4 regressions in a loop:
ff <- y ~ x
for(i in 1:4) {
 ff[2:3] <- lapply(paste(c("y","x"), i, sep=""), as.name)
 ## or   ff[[2]] <- as.name(paste("y", i, sep=""))
 ##      ff[[3]] <- as.name(paste("x", i, sep=""))
 assign(paste("lm.",i,sep=""), lmi <- lm(ff, data= anscombe))
 print(summary(lmi))
}
```

Ahora hagamos lo que deberíamos haber hecho antes de construir los modelos lineales: grafiquemos!: 

```{r}
op <- par(mfrow=c(2,2), mar=.1+c(4,4,1,1), oma= c(0,0,2,0))
for(i in 1:4) {
 ff[2:3] <- lapply(paste(c("y","x"), i, sep=""), as.name)
 plot(ff, data =anscombe, col="red", pch=21, bg = "orange", cex = 1.2,
      xlim=c(3,19), ylim=c(3,13))
 abline(get(paste("lm.",i,sep="")), col="blue")
}
mtext("Los 4 conjuntos de datos de Anscombe", outer = TRUE, cex=1.5)
par(op)
```

¿En cuál de los 4 conjuntos es válido utilizar un modelo lineal? 

## Predicciones

Los datos que queremos predecir deben estar en un data.frame. En el siguiente ejemplo, vamos a predecir 8 edades, entre 40 y 75 años: 

```{r}
new <- data.frame(Age = seq(40, 75, 5)) # = 40, 45, 50, 55, 60, 65, 70, 75 (las 8 edades)
predict(lm(BP ~ Age), new, se.fit = TRUE)
```

El resultado tiene dos partes: 
- las predicciones, o sea, las presiones sanguineas (155.0493 161.3035 167.5578 173.8120 180.0663 186.3205 192.5748 198.8290).
- los intervalos de confianza (3.316130 2.622421 2.082275 1.836585 1.997153 2.486483 3.155151 3.912257).

La información previa se puede guardar en arreglos de esta manera: 

```{r}
pred.w.plim <- predict(lm(BP ~ Age), new, interval="prediction", level = 0.95)
pred.w.clim <- predict(lm(BP ~ Age), new, interval="confidence", level = 0.95)
```

A modo de resumen, el siguiente gráfico muestra: 
  - la recta del modelo lineal (linea continua, negra).
  - el intervalo de confianza de la predicción (linea punteada colorada).
  - el intervalo de confianza del modelo lineal (linea punteada azul).
  - los 20 pacientes (puntos).
  
```{r}
matplot(new$Age,cbind(pred.w.clim, pred.w.plim[,-1]),
        lty=c(1,2,2,3,3),col=c("black","red","red","blue","blue"),type="l", ylab="predicted BP", xlab="Age")
points(Age,BP)
```

---

# Regresión lineal múltiple

En el ejemplo anterior predijimos la presión sanguinea (`PB`) utilizando la edad (`Age`): 

$PB = β_0 + β_1·Age + e$

Se trata de una regresión lineal *simple* porque se utiliza una sola variable (`Age`) para predecir la presión sanguinea. 

Ahora veremos un ejemplo en donde vamos a utilizar varias variables para predecir un valor. O sea: 

$X = β_0 + β_1·X_1 + ... + β_n·X_n + ... e$

Trabajaremos 11 variables de 22 pacientes:

```{r}
# Weight in kg
Mass<- c(77.0, 85.5, 63.0, 80.5, 79.5, 94.0, 66.0, 69.0, 65.0, 58.0, 69.5, 73.0, 74.0, 68.0, 80.0, 66.0, 54.5, 64.0, 84.0, 73.0, 89.0, 94.0)
# Maximum circumference of forearm in cm
Fore<-c(28.5, 29.5, 25.0, 28.5, 28.5, 30.5, 26.5, 27.0, 26.5, 26.5, 28.5, 27.5, 29.5, 25.0, 29.5, 26.5, 24.0, 25.5, 30.0, 28.0, 29.0, 31.0)
# Maximum circumference of bicep in cm
Bicep<- c(33.5, 36.5, 31.0, 34.0, 36.5, 38.0, 29.0, 31.0, 29.0, 31.0, 37.0, 33.0, 36.0, 30.0, 36.0, 32.5, 30.0, 28.5, 34.5, 34.5, 35.5, 33.5)
# Distance around chest directly under the armpits in cm
Chest<- c(100.0, 107.0, 94.0, 104.0, 107.0, 112.0, 93.0, 95.0, 93.0, 96.0, 109.5, 102.0, 101.0, 98.5, 103.0, 89.0, 92.5, 87.5, 99.0, 97.0, 106.0, 106.0)
# Distance around neck, approximately halfway up in cm
Neck<- c(38.5, 39.0, 36.5, 39.0, 39.0, 39.0, 35.0, 37.0, 35.0, 35.0, 39.0, 38.5, 38.5, 37.0, 40.0, 35.0, 35.5, 35.0, 40.5, 37.0, 39.0, 39.0)
# Distance around shoulders, measured around the peak of the shoulder blades in cm
Shoulder<- c(114.0, 119.0, 102.0, 114.0, 114.0, 121.0, 105.0, 108.0, 112.0, 103.0, 118.0, 113.0, 115.5, 108.0, 117.0, 104.5, 102.0, 109.0, 119.0, 104.0, 118.0, 120.0)
# Distance around waist, approximately trouser line in cm
Waist<- c(85.0,  90.5, 80.5, 91.5, 92.0, 101.0, 76.0, 84.0, 74.0, 76.0, 80.0, 86.0, 82.0, 82.0, 95.5, 81.0, 76.0, 84.0, 88.0, 82.0, 96.0, 99.5)
# Height from top to toe in cm
Height<- c(178.0, 187.0, 175.0, 183.0, 174.0, 180.0, 177.5, 182.5, 178.5, 168.5, 170.0, 180.0, 186.5, 188.0, 173.0, 171.0, 169.0, 181.0, 188.0, 173.0, 179.0, 184.0)
# Maximum circumference of calf in cm
Calf<- c(37.5, 40.0, 33.0, 38.0, 40.0, 39.5, 38.5, 36.0, 34.0, 35.0, 38.0, 36.0, 38.0, 37.0, 37.0, 38.0, 32.0, 35.5, 39.0, 38.0, 39.5, 42.0)
# Circumference of thigh, measured halfway between the knee and the top of the leg in cm
Thigh<- c(53.0, 52.0, 49.0, 50.0, 53.0, 57.5, 50.0, 49.0, 47.0, 46.0, 50.0, 49.0, 49.0, 49.5, 52.5, 48.0, 42.0, 42.0, 50.5, 49.0, 51.0, 55.0)
# Distance around head in cm
Head<- c(58.0, 59.0, 57.0, 60.0, 59.0, 59.0, 58.5, 60.0, 55.5, 58.0, 58.5, 59.0, 60.0, 57.0, 58.0, 56.5, 57.0, 58.0, 56.0, 58.0, 58.5, 57.0)

mydata<-cbind(Mass, Fore, Bicep, Chest, Neck, Shoulder, Waist, Height, Calf, Thigh, Head)
mydata<-as.data.frame(mydata)
mydata
```

Nuestro objetivo es construir una regresión lineal múltiple para predecir el peso (`Mass`).

## Gráfico de los datos

Primero, graficamos los datos:

```{r}
plot(mydata)
```

## Modelo con todos las variables

Ahora vamos a construir el modelo para predecir el peso (`Mass`) utilizando todas las variables:

```{r}
model_mult<-lm(Mass~., data=mydata);
model_mult
```

Lo único que cambió, respecto del ejemplo de la regresión lineal simple (`PB~Age`), es que utilizamos el operador `.` a la derecha de `~`, lo que significa "todos". Veamos las estadísticas del modelo: 

```{r}
summary(model_mult)
```

Algunos comentarios: 

* Los residuos son las diferencias entre los valores observados y predichos.
* La altura (Height) y la cintura (Waist) son estadísticamente significativas.
* La desviación estándar de los residuos es de 2.287.
* El estadístico F es muy significativo.
* Multiple R-squared = 0.9772 significa que el 97.72% de la variación del peso se explica por todas las variables.

## Modelo con algunas variables

Ahora vamos a probar un modelo en donde utilizaremos el antebrazo (`Fore`), la cintura (`Waist`) y la altura (`Height`). 

```{r}
summary(lm(Mass~Fore + Waist + Height, data = mydata))
```

## Datos correlados (opcional)

```{}
summary(lm(Mass~Fore, data = mydata))
summary(lm(Mass~Bicep, data = mydata))
summary(lm(Mass~Fore+Bicep, data = mydata))
```

> Porqué el bicep tiene significancia estadística si es la única variable, y deja de tenerla cuando está junto al antebrazo?

***Ayuda: probar `cor(mydata[,"Fore"],mydata[,"Bicep"])`***

## Interacciones (opcional)

```{r}
summary(lm(Mass~Fore + Waist + Height+Fore:Waist, data = mydata))
```


# Predicciones

Vamos a predecir el peso de 3 pacientes:

```{r}
new<-matrix(c(30.5, 28.5, 28.5, 33.5, 30.0, 31.0, 93.0, 112.0, 89.0, 39.0, 35.0, 38.5, 109.0, 105.0, 121.0, 84.0, 82.0, 96.0, 188.0, 177.5, 178.5, 37.5, 34.0, 40.0, 55.0, 42.0, 49.0, 58.0 , 59.0, 60.0),nrow=3,byrow=F)
colnames(new)<-colnames(mydata)[2:11];
new<-as.data.frame(new);
#Predicción
predict(model_mult, new, se.fit = TRUE)
```

Podemos exigir un intervalo de confianza del 95%: 

```{r}
predict(model_mult, new, interval="confidence", level = 0.95)
```

```{r}
predict(model_mult, new, interval="prediction", level = 0.95)
```

---

# Selección de variables

Hasta ahora, la selección de las variables que conformarán el modelo ha sido manual. Nosotros indicamos, por ejemplo, que el peso de paciente (´Mass´) se prediga utilizando el antebrazo (`Fore`), la cintura (`Waist`) y la altura (`Height`). 

En muchos casos, se presenta el problema de tener muchas variables y, por tanto, de no saber cuáles son importantes para el modelo. Vamos a ver un método que permite la selección automática del mejor subconjunto de variables para predecir una determinada variable. 

Recomendamos leer [este artículo](https://www.analyticsvidhya.com/blog/2016/12/introduction-to-feature-selection-methods-with-an-example-or-how-to-select-the-right-variables/) sobre la importancia de la selección de variables. 

## Introducción al método Stepwise (paso a paso)

El método Stepwise es un grupo de algoritmos que tienen como objetivo automatizar la selección de variables en un modelo.

Se dividen principalmente en: Selección hacia atrás (backward) y hacia adelante (forward)

* Eliminación hacia atrás (backward)

Este es el más simple de todos los procedimientos de selección de variables. De hecho, se puede implementar fácilmente. Pasos:

1. Construir un modelo con todas las variables.
2. Eliminar una de las variable (según un criterio determinado). 
3. Volver a construir el modelo y retorna al paso 2.
4. Deternerse cuando la solución converja.

* Selección hacia adelante (forward)

Es el método anterior invertido.

1. Comenzar sin variables en el modelo.
2. Aplicar un criterio específico.
3. Agrega una variable usando este criterio.
4. Volver a construir el modelo y retorna al paso 2.
5. Continuar hasta que no se puedan agregar nuevas variables.

Aplicaremos ambos métodos al ejemplo del peso. Para eso, construiremos dos modelos: 

* fit1: el modelo con todas las variables.
* fit2: el modelo conteniendo, solamente, el Intercept.

```{r}
fit1 <- lm(Mass ~ .,data=mydata)
fit2 <- lm(Mass ~ 1,data=mydata)
```

***Nota: Los métodos backward y forward no necesariamente convergen el mismo subconjunto de variables.***

### Ejemplo utilizando backward

```{r}
library(MASS)
#It is a local optimum.
modelAIC1<-stepAIC(fit1,direction="backward")
```

El método finaliza, mostrando el listado de variables seleccionadas, esto es `Mass ~ Fore + Waist + Height + Calf + Thigh + Head`. 

### Ejemplo utilizando forward

```{r}
#It is a local optimum.
modelAIC2<-stepAIC(fit2,direction="forward",scope=list(upper=fit1,lower=fit2))
```

El método finaliza, mostrando el listado de variables seleccionadas. En este caso, el resultado es el mismo que en el método anterior: `Mass ~ Fore + Waist + Height + Calf + Thigh + Head`. 

### Ejemplo utilizando `direction=both`

También es posible indicar que el método avance en ambas direcciones. Es decir: que confluya comenzando desde los dos extremos a un punto intermedio. 

```{r}
#It is a local optimum.
fit3 <- lm(Mass ~ .*.,data=mydata)
modelAIC3<-stepAIC(fit1,direction="both",scope=list(upper=fit3,lower=fit2))
```

> RESUMEN: La regresión lineal ordinaria predice el valor esperado de una cantidad desconocida dada (la variable de respuesta, una variable aleatoria) como una combinación lineal de un conjunto de valores observados (predictores). Esto implica que un cambio constante en un predictor conduce a un cambio constante en la variable de respuesta (es decir, un modelo de respuesta lineal). Esto es apropiado cuando la variable de respuesta tiene una distribución normal.

---

# Regresión logística

Las regresiones lineas son muy útiles y eficientes siempre y cuando las predicciones (los datos de respuesta) que deseamos seamos continuas. En el ejemplo anterior, el peso (`Mass`) es una variable continua. De hecho, los valores predichos pertenecen a una recta (función continua).

Las ***regresiones logísticas*** se utilizan cuando necesitamos predicciones categóricas, binarias. Por ejemplo: "SI/NO", "SANO/ENFERMO", "TIENE/NO TIENE".

Ejemplo: 

Utilizando un dataset de pacientes de EEUU (32968 pacientes y 36 variables de cada paciente), vamos a predecir la probabilidad de tener hipertensión utilizando, como variables independientes: edad (age), sexo (sex), tiempo de sueño (sleep) e índice de masa corporal (bmi). 

Pasos: 

1. Descargar el archivo `LogisticRegresion&StepAIC.RData` a tu directorio de trabajo

2. Cargar los datos 

``` {r}
load("./resources/LogisticRegresion&StepAIC.RData")
```

3. Ver el contenido de cada columna: 

```{r, echo=FALSE}
labs <- attributes(NH11)$labels
knitr::kable(labs)
```

4. Chequear la estructura de `hypev`

```{r}
  str(NH11$hypev) # check stucture of hypev
```

5. Chequear los niveles de `hypev`

```{r}
  levels(NH11$hypev) # check levels of hypev
```

6. Borrar los datos NA

```{r}
NH11<-NH11[which(!is.na(NH11$hypev)),]
```

7.  Construir la regresión logística

```{r}
  hyp.out <- glm(hypev~age_p+sex+sleep+bmi,
                data=NH11, family="binomial")
  summary(hyp.out)
  coef(summary(hyp.out))
```

8. Validar

Ahora que tenemos el modelo es importante saber si "predice bien". Dos propuestas: 

a. ***Propuesta 1:*** Aplicar el modelo a los 32968 pacientes y ver si la predicción de cada uno coincide con el campo `NH11$hypev`. El porcentaje de coincidencia podría ser un índice de calidad del modelo. 

<span style="color:blue"> Esta propuesta es, en realidad, engañosa ya que estamos probando el predictor en los mismos datos que utilizamos para ***entrenar*** el modelo. *Es hacer un poco de trampa*. De ahí que esta propuesta, en el ámbito de la ciencia de datos, no es muy aceptable. </span>

b. ***Propuesta 2:*** Antes de construir el modelo (o sea, entre el paso 6 y el 7), se divide el dataset en dos: `train` y `test` (se puede hacer un split 70-30, por ejemplo). La construcción del modelo (paso 7) se realiza con los datos de train y la validación se realiza con los datos de `test`.

<span style="color:blue"> Esta propuesta es la que se utiliza de modo habitual.</span>

Para nuestro ejemplo, vamos a aplicar la ***propuesta a.*** (o sea: aplicaremos el modelo a los 32968 pacientes que usamos para el entrenamiento).  Predicción de la hipertensión de todos los pacientes: 

```{r}
predsAll<-predict(hyp.out, type = "response")

boxplot(predsAll ~ NH11$hypev, col = c("green", "red"),
        ylab = "Probabilidad",
        xlab = "Tiene / No tiene hypertensión")

plot(density(predsAll[which(NH11$hypev=="1 Yes")]), col ="dark green", main = "Density functions", ylim=c(0, 5) )
lines(density(predsAll[which(NH11$hypev=="2 No")]), col ="red", main = "Density functions")

```

9. Hacer predicciones

Por ejemplo, podemos preguntar "¿Cuánto más probable es que una mujer de 63 años tenga hipertensión en comparación con una mujer de 33 años?".

```{r}
  # Create a dataset with predictors set at desired levels
  predDat <- with(NH11,
                  expand.grid(age_p = c(33, 63),
                              sex = "2 Female",
                              bmi = mean(bmi, na.rm = TRUE),
                              sleep = mean(sleep, na.rm = TRUE)))
  # predict hypertension at those levels

preds <- predict(hyp.out, type = "response",
                         se.fit = TRUE, interval="confidence",
                         newdata = predDat)
  cbind(predDat, preds)
```

El resultado indica que una mujer de 33 años tiene un 13% de probabilidad de haber sido diagnosticada con hipertensión, mientras que una mujer de 63 años tiene un 48%. 


---

# Modelo lineal generalizado

John Nelder y Robert Wedderburn formularon modelos lineales generalizados como una forma de unificar otros modelos estadísticos, como la regresión lineal, la regresión logística y la regresión de Poisson. 

Los modelos lineales generalizados cubren todas estas situaciones al permitir variables de respuesta que tienen distribuciones arbitrarias (en lugar de simplemente distribuciones normales), y que una función arbitraria de la variable de respuesta (la función de enlace) varíe linealmente con los valores predichos. 

---

# Particionamiento Recursivo

Vamos a realizar una predicción construyendo un modelo de clasificación o regresión, del [paquete rpart](http://127.0.0.1:13413/help/library/rpart/doc/longintro.pdf). El modelo resultante se puede representar como un árbol binario.

Estos árboles de decisión construyen el modelo con técnicas de machine learning. Seleccionan la variable que mejor divide en dos a los datos, y la utiliza como raíz del árbol de decisión. Luego repite esa misma acción en cada una de las ramas hasta un cierto corte. 

En esta primera instancia conviene saber que `rpart` admite dos criterios de división de los datos: a) el índice de Gini y b) el índice de información. A los efectos de este curso, usaremos el índice de Gini. 

Para entender el funcionamiento, veremos un ejemplo. 

Vamos a utilizar los datos un dataset de R (llamado `cu.summary`) que contiene el grado de confiabilidad de 117 autos. 

```{r}
library(rpart)
str(cu.summary)
```

Las variables son: 

|Variable    | Descripción                                                    |
|:-----------|:---------------------------------------------------------------|
|Reliability | variable tipo factor (contiene NAs)                               |
|            | Much worse < worse < average < better < Much Better            |
|Price       | numérico: precio                                               |
|Country     |  pais donde fue fabricado                        |
|Mileage     | tamaño del tanque. Contiene NAs                      |
|Type        | Tipo: Small, Sporty, Compact, Medium, Large, Van             |

Vamos a predecir la ***confiabilidad*** (Reliability).

1. Veamos cómo son los datos de la variable de salida: 

```{r}
table(cu.summary$Reliability)
```

Hay 32 autos que no tienen indicado un nivel de confiabilidad: 

```{r}
table(is.na(cu.summary$Reliability))
```

2. Consutrcción del modelo (usando el índice de Gini) 

```{r}
fit1 <- rpart(Reliability ~ Price + Country + Mileage + Type, data = cu.summary, parms = list(split = 'gini'))
```

3. Visualización del árbol correspondiente (usando el paquete ´rattle´):

```{r}
library(rattle)
fancyRpartPlot(fit1)
```

Cómo leer el arbol? Vamos a interpretar los números del nodo raíz (nodo 1): 

- Hay un 21 % de "Much worse", 14 % de "worse", 31 % de "average", 9 % de "better" y 25 % de "Much Better" de autos, antes de hacer cualquier split. 
- "average" significa que el arbol eligió votar "en promedio".
- 100% significa que los porcentajes indicados corresponden a totalidad de la muestra. 

- Luego, si el auto fue fabricado en Brazil o Inglaterra, o Francia o Japón (nodo 3), hay un 11 % que sea "average", 11 % que sea "better" y un 78 % de que sea "Much Better". 
- El rótulo "Much better" del nodo 3 indica que, los autos que caigan en esta hoja del árbol serán calificados de "Much better".

- Los valores de los rótulos de las hojas indican la votación que el modelo hace de los casos que caigan en esa hoja. 

4. Para mayor conocimiento de los resultados: 

```{r}
summary(fit1)
```

5. Ahora que tenemos nuestro modelo, vamos a predecir la confiabilidad de los 32 autos que tienen NA en dicha columna: 

```{r}
# nos quedamos con los autos sin calificar
AutosSinCalificar<-cu.summary[is.na(cu.summary$Reliability),]

# realizamos las predicciones
Predictions <- predict(fit1, AutosSinCalificar, type = "class")

# copiamos las predicciones en la columna del data.frame
AutosSinCalificar$Reliability<-Predictions
knitr::kable(AutosSinCalificar)
```

6. ¿Cómo sabemos si las predicciones son buenas? Vamos a aplicar el predictor a los 85 autos y comparar el valor predicho con el valor indicado en el data.frame:

```{r}
allcars<-cu.summary
allcars<-allcars[!is.na(allcars$Reliability),]

predsAll <- predict(fit1, allcars, type = "class")

# Restamos predicho - indicado y lo sumarizamos
table(abs(as.numeric(predsAll)-as.numeric(allcars$Reliability)))

hist(abs(as.numeric(predsAll)-as.numeric(allcars$Reliability)), 
     main = "", xlab = "predsAll-allcars$Reliability", ylab = "cantidad de autos", 
     col="lightcyan", shadow=TRUE)
```

De los 85 autos, el arbol de regresión ha predicho:

- correctamente el 63.5 % de los autos (54/85)
- se ha equivocado por 1 categoría en el 18.82 % de los autos (16/85)
- se ha equivocado por 2 categorías en el 17.64 % de los autos (15/85)

Tarea: pruebe la performance del predictor cambiando el criterio de split (no utilizar 'gini' y utilizar 'information')

# Machine learning con el paquete `CARET`

Uno de los mayores retos del aprendizaje automático es acertar en los algoritmos adecuados y configurar los parámetros de manera adecuada. 

El paquete `caret` (Classification And Regression Training) es posiblemente uno de los proyectos de R más grandes de *machine learning*. Este paquete contiene todo lo que necesitas saber para resolver casi cualquier problema de aprendizaje automático supervisado. Proporciona una interfaz uniforme para varios algoritmos de aprendizaje automático y estandariza otras tareas como la división de datos, el preprocesamiento, la selección de características, la estimación de importancia de variables, etc. Es esencialmente un *wrapper* que contiene más de 200 algoritmos de machine learning. 

Vamos a recorrer algunas funcionalidades de `caret` utilizando el dataset [The Loan Prediction problem-III](https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/).

1. Descargar el paquete

```{}

> install.packages("caret", dependencies = c("Depends", "Suggests"))
```

2. Cargar el paquete y los datos

```{r}
library(caret)

# Subir los datos del Loan prediction problem III:
train<-read.csv("./resources/train_u6lujuX_CVtuZ9i.csv",stringsAsFactors = T)

# Ver la estructura de los datos:
str(train)
```

En nuestro caso, vamos a predecir el status de préstamo (`Loan_Status`) basándonos en los datos de la persona. 

3. Pre-procesado de los datos

CARET necesita que: 
- no haya valores nulos. 
- la variable dependiente debe ser numérica. 
- que las variables independientes sean categóricas (factores).
- que los valores de las categorías sean numéricos.

En los próximos pasos, trabajaremos los datos para cumplir con esos requisitos. 

3.1 Reemplazar valores nulos:

```{r}
sum(is.na(train))
```

Vamos a completar los valores nulos usando el [algoritmo KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). Dado un valor  x, KNN busca  entre los "vecinos cercanos" de x, cuál es la clasificación (o un determinado valor) más frecuente. 

En nuestro caso, le vamos a pedir a KNN que "centre" y "escale" los valores nalus respecto de sus vecinos. 

```{r}
preProcValues <- preProcess(train, method = c("knnImpute","center","scale"))

library(RANN)
train_processed <- predict(preProcValues, train)
sum(is.na(train_processed))
```

3.2 Convertir la variable dependiente (`Loan_Status`) a numérica.

```{r}
train_processed$Loan_Status<-ifelse(train_processed$Loan_Status=='N',0,1)

id<-train_processed$Loan_ID
train_processed$Loan_ID<-NULL

# Chequear la estructura del data.frame preprocesado
str(train_processed)
```

3.3 Convertir cada variable categórica a numérica usando variables ficticias (dummy)

```{r}
dmy <- dummyVars(" ~ .", data = train_processed,fullRank = T)
train_transformed <- data.frame(predict(dmy, newdata = train_processed))

# Chequear la estructura del data.frame transformado
str(train_transformed)
```

Aquí, `fullrank = T` creará solo (n-1) columnas para una columna categórica con n niveles diferentes. Esto funciona especialmente bien para los predictores categóricas como sexo, si está casado, etc. donde solo tenemos dos niveles: masculino/femenino, sí/no, etc. porque 0 se puede usar para representar una clase mientras que 1 representa la otra clase en la misma columna.

3.4 Volver a convertir las variables a categóricas

```{r}
train_transformed$Loan_Status<-as.factor(train_transformed$Loan_Status)
```

4. Dividir los datos entre train y test

Vamos a dividir los datos entre 75 % y 25 %: 

```{r}
index <- createDataPartition(train_transformed$Loan_Status, p=0.75, list=FALSE)
trainSet <- train_transformed[ index,]
testSet <- train_transformed[-index,]
```

5. Seleccionar de variables (feature selection)

Vamos a realizar una selección automática de variables con la técnica RFE (recursive feature).

```{r}
control <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 3,
                   verbose = FALSE)

outcomeName<-'Loan_Status'

predictors<-names(trainSet)[!names(trainSet) %in% outcomeName]

Loan_Pred_Profile <- rfe(trainSet[,predictors], trainSet[,outcomeName],
                      rfeControl = control)

Loan_Pred_Profile
```

El selector sugiere utilizar 5 variables: "Credit_History", "LoanAmount", "Loan_Amount_Term", "ApplicantIncome", "CoapplicantIncome" que guardaremos en: 

```{r}
predictors<-c("Credit_History", "LoanAmount", "Loan_Amount_Term", "ApplicantIncome", "CoapplicantIncome")
```

6. Generar el modelo

Caret tiene +200 modelo estadístico de predicción. Puedes verlos con:

```{r}
names(getModelInfo())
```

Para conocer el funcionamiento, uso y parámetros de cada modelo, puedes visitar [esta página](http://topepo.github.io/caret/available-models.html). 

Nosotros vamos a generar 4 modelos: 

```{r echo=T, results='hide'}
# modelo Random Forest
model_rf<-train(trainSet[,predictors],trainSet[,outcomeName],method='rf')

# modelo con una red neuronal
model_nnet<-train(trainSet[,predictors],trainSet[,outcomeName],method='nnet')

# modelo lineal generalizado
model_glm<-train(trainSet[,predictors],trainSet[,outcomeName],method='glm')

# modelo de incremento estocástico del gradiente
model_gbm<-train(trainSet[,predictors],trainSet[,outcomeName],method='gbm')
```


7. Configuración de parámetros (Parameter tuning)

Como hemos comentado, Caret contiene más de 200 modelos. Cada uno de ellos contiene sus propio set de parámetros y, por tanto, es necesario setearlos correctamente. A continuación incluimos el tradicional pseudo-código que se utiliza en Caret para el seteo de parámetros: 

![](https://www.analyticsvidhya.com/wp-content/uploads/2016/12/caret-6.png)

> **Resampling**: método estadístico que se utiliza para medir la performance del modelo seleccionado. Existen varios métodos de resampling. Caret utiliza [`bootstrap`](https://es.wikipedia.org/wiki/Bootstrapping_(estadística)) por default.  

En nuestro ejemplo, vamos a utilizar validación cruzada (`cross-validation`) para el resampling. Vamos a repetir 5 veces, una validación cruzada con 5 participanes (*5-Fold cross-validation repeated 5 times*): 

```{r}
fitControl <- trainControl(
  method = "repeatedcv",
  number = 5,
  repeats = 5)
```

6.1 Conocer los parámetros de un modelo

Para ver los parámetros que requiere un modelo, se utiliza la función `modelLookup()`. Por ejemplo, para conocer los parámetro del modelo de incremento estocástico del gradiente (gbm);

```{r}
modelLookup(model='gbm')
```

6.2 Grilla de valores para los parámetros

Vamos a generar una grilla con posibles valores de manera que Caret nos sugiera con cuáles quedarnos: 

```{r}
#Generación de una grilla
grid <- expand.grid(n.trees=c(10,20,50,100,500,1000),shrinkage=c(0.01,0.05,0.1,0.5),n.minobsinnode = c(3,5,10),interaction.depth=c(1,5,10))
```

6.3 Generación de escenarios a partir de la grilla

```{r echo=T, results='hide'}
# training the model
model_gbm<-train(trainSet[,predictors],trainSet[,outcomeName],method='gbm',trControl=fitControl,tuneGrid=grid)

print(model_gbm)
```

En nuestro caso, utilizado el criterio de Precisión (Accuracy) para seleccionar el mejor set de valores. Los valores más conveniente que resultan del análisis son: 

- n.trees = 10,
- interaction.depth = 1, 
- shrinkage = 0.05 y (contracción)
- n.minobsinnode = 3

6.4 Veamos las gráficas de cada escenario

```{r}
plot(model_gbm)
```

6.5 `tuneLength` en lugar de definir una grilla de valores

En lugar de especificar los valores exactos de cada parámetro para realizar el *tuning* de los parámetros, podemos utilizar el parámetro `tuneLength` que busca cualquier número de posibles valores por cada *tuning* de parámetro. Vamos a probar utilizando `tuneLength=10`: 

```{r echo=T, results='hide'}
model_gbm<-train(trainSet[,predictors],trainSet[,outcomeName],method='gbm',trControl=fitControl,tuneLength=10)
print(model_gbm)
```

Interpretación: 

- El parámetro `shrinkage` se mantuvo en 0.1, 
- El parámetro `n.minobsinnode´ se mantuvo constante en 10,
- La precisión (Accuracy) se utilizó como criterio para seleccionar el modelo óptimo.

Los valores finales sugeridos son: 

- n.trees = 50, 
- interaction.depth = 2, 
- shrinkage = 0.1 y 
n.minobsinnode = 10.

6.5.1 Gráfica de la propuesta: 

```{r}
plot(model_gbm)
```

7. Estimación de la importancia de las variables

Una vez que se han seleccionado las variables, puede ser conveniente indicar cuán importante es cada variable en el modelo. En `caret` se puede hacer del siguiente modo: 

```{r}
library(gbm)
varImp(object=model_gbm)
plot(varImp(object=model_gbm),main="GBM - Variable Importance")
```

Sugerimos realizar la estimación de importancia para los otros tres modelos: ¿el ranking de importancia de variables es la misma para los 4 modelos?

8. Predicciones

Las predicciones las podemos realizar con la función `predict.train()`. Se deberá indicar el modelo y los datos de test. Cuando se trata de modelos de clasificación, Caret permite setear un el parámetro type con dos posibles valores: 

- `type="raw"`: la salida será la predicción "cruda". El valor (sin más).
- `type="prob"`: la salida proporcionará las probabilidades de ocurrencia de cada observación en varias clases de variable de salida.  

```{r echo=T, results='hide'}
predictions<-predict.train(object=model_gbm,testSet[,predictors],type="raw")
table(predictions)
```

9. Exactitud del modelo

En inteligencia artificial, una ***matriz de confusión*** indica el desempeño de un algoritmo en un proceso de aprendizaje supervisado. Las columnas son datos reales y las filas son datos predichos. En el siguiente ejemplo, de 8 gatos reales, el modelo predice correctamente a 5 y a los otros tres los confunde con un perro: 

![](https://github.com/icassol/platform-samples/blob/master/Confusion.jpg?raw=true)

Sobre la base de la matriz de confusión, es simple medir la exactitud de un modelo: 

```{r}
confusionMatrix(predictions,testSet[,outcomeName])
```



