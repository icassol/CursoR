---
title: 'Regresión lineal simple y múltiple'
output:
  html_document:
    fig_caption: yes
    fig_height: 4
    fig_width: 4.5
    number_sections: yes
    toc: yes
    toc_depth: 2
---

<!-- see http://rmarkdown.rstudio.com/ for details in formatting -->
```{r style, echo = FALSE, results='hide', message=FALSE, warning=FALSE}
if (!"BiocStyle" %in% rownames(installed.packages()))
  BiocManager::install("BiocStyle")
library(BiocStyle)
BiocStyle::markdown()
options(width=100)
knitr::opts_chunk$set(cache=TRUE, autodep=TRUE)
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Regresiones lineales 

Teniendo un conjunto de datos, y suponiendo que cada dato es un par ordenado (x,y), una ***regresión lineal simple*** genera una recta cuyos puntos (x',y') tienen la mínima distancia posible respecto a (x,y). 

El modelo anterior implica que la regresión lineal simple considera a uno de los ejes de coordenadas como variable dependiente y al otro como variable independiente. 

Veamos un ejemplo y luego seguiremos explicando los conceptos: 

Trabajaremos sobre una muestra de 20 pacientes. De cada uno de ellos sabemos su presión sanguinea (BP: blood pressure) y su edad (Age). Es decir: 

  - el conjunto de datos son los 20 pacientes. 
  - el par ordenado (x,y) es (BP, Age).
  - el objetivo es generar una recta cuyos puntos (x',y') tengan la menor distancia respecto de (x,y). Al valor y' se le llama **predicción**. 

Carguemos y grafiquemos los datos: 

```{r cars, message=FALSE, warning=FALSE}
Age<-c(67, 40, 68, 64, 47, 71, 54, 59, 46, 52, 57, 63, 45, 55, 48, 51, 48, 77, 38, 61)
BP<-c(185, 154, 198, 189, 168, 200, 166, 190, 155, 172, 180, 173, 176, 172, 150, 176, 155, 192, 154, 185)

plot(Age,BP, pch =16); # seteamos puntos rellenos

```

La nube de puntos sigue una forma lineal (es un comportamiento similar a una recta). 

Contando con los arreglos `BP` y `Age` es posible saber, por ejemplo, que un paciente de 67 años, tiene una presión sanguínea de 185. O sea, la pregunta 

> *¿qué presión tendrá un paciente con una edad determinada?*

la podremos hacer 20 veces (preguntando por cada edad del arreglo `Age`).

La idea de la regresión lineal es poder generalizar la pregunta anterior a cualquier edad. Esa información, precisamente, es la que contiene la recta. De esta manera, contando con la regresión lineal podremos **predecir** qué presión sanguinea tendrá, por ejemplo, una persona de 53 años (53 es una edad que no existe en el arreglo `Age`).

La recta que "pasará cerca" de los 20 puntos tendrá como variable indepediente la edad, y como variable dependiente la presión sanguinea. 

En R es muy simple generar la regresión lineal: 

```{r}
model<-lm(BP~Age);
coef(model);
```

El formato del parámetro del comando `lm()` es `lm([columna a predecir] ~ [término/s que se utilizarán para construir el modelo lineal])`

El valor independiente de la recta será el `Intercept` y la pendiente será `Age`. Habitualmente al Intercept se le dice B0 y a la pendiente B1. 

Tarea: ¿qué información aportan los valores B0 y B1? 

Es posible conocer valores estadísticos del modelo: 

```{r}
summary(model)
```

Este resumen contiene información muy valiosa acerca de la calidad del modelo. Para este curso, por el momento, destacaremos que:

- B0 y B1 -ambos- son "muy significativos estadísticamente" ya que sus p-values (4.44e-09 y 1.44e-09) son muy pequeños. 
- El estadístico F también es muy significativo (1.439e-06).

# Residuos

Si (x', y') es un punto de la recta del modelo lineal, y (x, y) es un punto de la población, existe una diferencia entre la observación (y) y la predicción (y') llamada ***residuo***.

***Validación de la regresión***. El análisis de los residuos del modelo lineal indica si el modelo representa correctamente a la muestra. A los efectos prácticos, para que una muestra pueda ser modelada por una regresión lineal, se deben cumplir dos requisitos: 

1. La muestra no debe tener **outliers** (valores atípicos). No debe haber puntos que estén lejanos a la nube de puntos que se asemeja -en conjunto- a una recta. Este requisito lo vamos chequear visualmente: se plotean los datos y se observan los puntos. 

2. Los residuos deben ser independientes y seguir una distribución normal. Este requisito se puede chequear con esta instrucción: 

```{r}
model.stdres=rstandard(model)

qqnorm(model.stdres,
  ylab="Standardized Residuals",
  xlab="Normal Scores")

qqline(model.stdres)

```

Si los residuos siguen una distribución normal, se espera que `qqline` sea recta. En este caso el resultado es bueno.

## Test de Shapiro

Para comprobar si un conjunto de datos siguen una distribución normal, se puede usar el test de Shapiro. Este test, entonces, lo podemos aplicar al conjunto de residuos del modelo lineal de manera que sepamos si, efectivamente, el modelo es válido para representar a la muestra. Por ejemplo, para una distribución normal:

```{r}
set.seed(5)
xtest <-  rnorm(100, mean = 5, sd = 2)
shapiro.test(xtest)
```

El p-value no es significativo (0.445) y, por lo tanto, aceptamos la hipótesis nula (los datos siguen una distribución normal).

Sin embargo, si los datos no siguen una distribución normal, entonces el test de Shapiro da un p-value pequeño y, por lo tanto, rechazamos la hipótesis nula. Ejemplo: 

```{r}
set.seed(5)
xtest <-  rchisq(100, 3) # distribución Chi-Cuadrado
shapiro.test(xtest)
```

## Ejemplo del impacto de los outliers

Esta sección apunta a mostrar la importancia de identificar los outliers (valores atípicos)  para utilizar una regresión lineal como modelo de representación de la muestra (el primer requisito indicado en la sección ***Residuos***). 

En 1973, el estadístico Francis Anscombe, diseñó un ejemplo con cuatro conjuntos de datos que tienen estadísticas descriptivas simples casi idénticas, pero que parecen muy diferentes cuando se grafican. Cada conjunto de datos consta de once (x, y) puntos. Este ejemplo sirve para mostrar:
  - la importancia de graficar los datos antes de analizarlos 
  - el efecto de los valores atípicos (outliers) en las propiedades estadísticas.

```{r}
anscombe
apply(anscombe,2,var) # la varianza de cada columna
apply(anscombe,2,mean) # la media de cada columna
```

Se ven muy similares. Construyamos una regresión lineal para los 4 conjuntos: 

```{r}
coef(lm(y1~x1,data=anscombe))
coef(lm(y2~x2,data=anscombe))
coef(lm(y3~x3,data=anscombe))
coef(lm(y4~x4,data=anscombe))
```

Las cuatro rectas son idénticas. 

Generemos los cuatro modelos de manera más eficiente, guardándolos en una lista: 

```{r}
summary(anscombe)

##-- now some "magic" to do the 4 regressions in a loop:
ff <- y ~ x
for(i in 1:4) {
 ff[2:3] <- lapply(paste(c("y","x"), i, sep=""), as.name)
 ## or   ff[[2]] <- as.name(paste("y", i, sep=""))
 ##      ff[[3]] <- as.name(paste("x", i, sep=""))
 assign(paste("lm.",i,sep=""), lmi <- lm(ff, data= anscombe))
 print(summary(lmi))
}
```

Ahora hagamos lo que deberíamos haber hecho antes de construir los modelos lineales: grafiquemos!: 

```{r}
op <- par(mfrow=c(2,2), mar=.1+c(4,4,1,1), oma= c(0,0,2,0))
for(i in 1:4) {
 ff[2:3] <- lapply(paste(c("y","x"), i, sep=""), as.name)
 plot(ff, data =anscombe, col="red", pch=21, bg = "orange", cex = 1.2,
      xlim=c(3,19), ylim=c(3,13))
 abline(get(paste("lm.",i,sep="")), col="blue")
}
mtext("Los 4 conjuntos de datos de Anscombe", outer = TRUE, cex=1.5)
par(op)
```

¿En cuál de los 4 conjuntos es válido utilizar un modelo lineal? 

## Predicciones

Los datos que queremos predecir deben estar en un data.frame. En el siguiente ejemplo, vamos a predecir 8 edades, entre 40 y 75 años: 

```{r}
new <- data.frame(Age = seq(40, 75, 5)) # = 40, 45, 50, 55, 60, 65, 70, 75 (las 8 edades)
predict(lm(BP ~ Age), new, se.fit = TRUE)
```

El resultado tiene dos partes: 
- las predicciones, o sea, las presiones sanguineas (155.0493 161.3035 167.5578 173.8120 180.0663 186.3205 192.5748 198.8290).
- los intervalos de confianza (3.316130 2.622421 2.082275 1.836585 1.997153 2.486483 3.155151 3.912257).

La información previa se puede guardar en arreglos de esta manera: 

```{r}
pred.w.plim <- predict(lm(BP ~ Age), new, interval="prediction", level = 0.95)
pred.w.clim <- predict(lm(BP ~ Age), new, interval="confidence", level = 0.95)
```

A modo de resumen, el siguiente gráfico muestra: 
  - la recta del modelo lineal (linea continua, negra).
  - el intervalo de confianza de la predicción (linea punteada colorada).
  - el intervalo de confianza del modelo lineal (linea punteada azul).
  - los 20 pacientes (puntos).
  
```{r}
matplot(new$Age,cbind(pred.w.clim, pred.w.plim[,-1]),
        lty=c(1,2,2,3,3),col=c("black","red","red","blue","blue"),type="l", ylab="predicted BP", xlab="Age")
points(Age,BP)
```

---

# Regresión lineal múltiple

En el ejemplo anterior predijimos la presión sanguinea (`PB`) utilizando la edad (`Age`): 

$PB = β_0 + β_1·Age + e$

Se trata de una regresión lineal *simple* porque se utiliza una sola variable (`Age`) para predecir la presión sanguinea. 

Ahora veremos un ejemplo en donde vamos a utilizar varias variables para predecir un valor. O sea: 

$X = β_0 + β_1·X_1 + ... + β_n·X_n + ... e$

Trabajaremos 11 variables de 22 pacientes:

```{r}
# Weight in kg
Mass<- c(77.0, 85.5, 63.0, 80.5, 79.5, 94.0, 66.0, 69.0, 65.0, 58.0, 69.5, 73.0, 74.0, 68.0, 80.0, 66.0, 54.5, 64.0, 84.0, 73.0, 89.0, 94.0)
# Maximum circumference of forearm in cm
Fore<-c(28.5, 29.5, 25.0, 28.5, 28.5, 30.5, 26.5, 27.0, 26.5, 26.5, 28.5, 27.5, 29.5, 25.0, 29.5, 26.5, 24.0, 25.5, 30.0, 28.0, 29.0, 31.0)
# Maximum circumference of bicep in cm
Bicep<- c(33.5, 36.5, 31.0, 34.0, 36.5, 38.0, 29.0, 31.0, 29.0, 31.0, 37.0, 33.0, 36.0, 30.0, 36.0, 32.5, 30.0, 28.5, 34.5, 34.5, 35.5, 33.5)
# Distance around chest directly under the armpits in cm
Chest<- c(100.0, 107.0, 94.0, 104.0, 107.0, 112.0, 93.0, 95.0, 93.0, 96.0, 109.5, 102.0, 101.0, 98.5, 103.0, 89.0, 92.5, 87.5, 99.0, 97.0, 106.0, 106.0)
# Distance around neck, approximately halfway up in cm
Neck<- c(38.5, 39.0, 36.5, 39.0, 39.0, 39.0, 35.0, 37.0, 35.0, 35.0, 39.0, 38.5, 38.5, 37.0, 40.0, 35.0, 35.5, 35.0, 40.5, 37.0, 39.0, 39.0)
# Distance around shoulders, measured around the peak of the shoulder blades in cm
Shoulder<- c(114.0, 119.0, 102.0, 114.0, 114.0, 121.0, 105.0, 108.0, 112.0, 103.0, 118.0, 113.0, 115.5, 108.0, 117.0, 104.5, 102.0, 109.0, 119.0, 104.0, 118.0, 120.0)
# Distance around waist, approximately trouser line in cm
Waist<- c(85.0,  90.5, 80.5, 91.5, 92.0, 101.0, 76.0, 84.0, 74.0, 76.0, 80.0, 86.0, 82.0, 82.0, 95.5, 81.0, 76.0, 84.0, 88.0, 82.0, 96.0, 99.5)
# Height from top to toe in cm
Height<- c(178.0, 187.0, 175.0, 183.0, 174.0, 180.0, 177.5, 182.5, 178.5, 168.5, 170.0, 180.0, 186.5, 188.0, 173.0, 171.0, 169.0, 181.0, 188.0, 173.0, 179.0, 184.0)
# Maximum circumference of calf in cm
Calf<- c(37.5, 40.0, 33.0, 38.0, 40.0, 39.5, 38.5, 36.0, 34.0, 35.0, 38.0, 36.0, 38.0, 37.0, 37.0, 38.0, 32.0, 35.5, 39.0, 38.0, 39.5, 42.0)
# Circumference of thigh, measured halfway between the knee and the top of the leg in cm
Thigh<- c(53.0, 52.0, 49.0, 50.0, 53.0, 57.5, 50.0, 49.0, 47.0, 46.0, 50.0, 49.0, 49.0, 49.5, 52.5, 48.0, 42.0, 42.0, 50.5, 49.0, 51.0, 55.0)
# Distance around head in cm
Head<- c(58.0, 59.0, 57.0, 60.0, 59.0, 59.0, 58.5, 60.0, 55.5, 58.0, 58.5, 59.0, 60.0, 57.0, 58.0, 56.5, 57.0, 58.0, 56.0, 58.0, 58.5, 57.0)

mydata<-cbind(Mass, Fore, Bicep, Chest, Neck, Shoulder, Waist, Height, Calf, Thigh, Head)
mydata<-as.data.frame(mydata)
mydata
```

Nuestro objetivo es construir una regresión lineal múltiple para predecir el peso (`Mass`).

## Gráfico de los datos

Primero, graficamos los datos:

```{r}
plot(mydata)
```

## Modelo con todos las variables

Ahora vamos a construir el modelo para predecir el peso (`Mass`) utilizando todas las variables:

```{r}
model_mult<-lm(Mass~., data=mydata);
model_mult
```

Lo único que cambió, respecto del ejemplo de la regresión lineal simple (`PB~Age`), es que utilizamos el operador `.` a la derecha de `~`, lo que significa "todos". Veamos las estadísticas del modelo: 

```{r}
summary(model_mult)
```

Algunos comentarios: 

* Los residuos son las diferencias entre los valores observados y predichos.
* La altura (Height) y la cintura (Waist) son estadísticamente significativas.
* La desviación estándar de los residuos es de 2.287.
* El estadístico F es muy significativo.
* Multiple R-squared = 0.9772 significa que el 97.72% de la variación del peso se explica por todas las variables.

## Modelo con algunas variables

Ahora vamos a probar un modelo en donde utilizaremos el antebrazo (`Fore`), la cintura (`Waist`) y la altura (`Height`). 

```{r}
summary(lm(Mass~Fore + Waist + Height, data = mydata))
```

## Datos correlados (opcional)

```{}
summary(lm(Mass~Fore, data = mydata))
summary(lm(Mass~Bicep, data = mydata))
summary(lm(Mass~Fore+Bicep, data = mydata))
```

> Porqué el bicep tiene significancia estadística si es la única variable, y deja de tenerla cuando está junto al antebrazo?

***Ayuda: probar `cor(mydata[,"Fore"],mydata[,"Bicep"])`***

## Interacciones (opcional)

```{r}
summary(lm(Mass~Fore + Waist + Height+Fore:Waist, data = mydata))
```


# Predicciones

Vamos a predecir el peso de 3 pacientes:

```{r}
new<-matrix(c(30.5, 28.5, 28.5, 33.5, 30.0, 31.0, 93.0, 112.0, 89.0, 39.0, 35.0, 38.5, 109.0, 105.0, 121.0, 84.0, 82.0, 96.0, 188.0, 177.5, 178.5, 37.5, 34.0, 40.0, 55.0, 42.0, 49.0, 58.0 , 59.0, 60.0),nrow=3,byrow=F)
colnames(new)<-colnames(mydata)[2:11];
new<-as.data.frame(new);
#Predicción
predict(model_mult, new, se.fit = TRUE)
```

Podemos exigir un intervalo de confianza del 95%: 

```{r}
predict(model_mult, new, interval="confidence", level = 0.95)
```

```{r}
predict(model_mult, new, interval="prediction", level = 0.95)
```

---

# Selección de variables

Ahora ahora, la selección de las variables que conformarán el modelo ha sido manual. Nosotros indicamos, por ejemplo, que el peso de paciente (´Mass´) se prediga utilizando el antebrazo (`Fore`), la cintura (`Waist`) y la altura (`Height`). 

En muchos casos, se presenta el problema de tener muchas variables y, por tanto, de no saber cuáles son importantes para el modelo. Vamos a ver un método que permite la selección automática del mejor subconjunto de variables para predecir una determinada variable. 

## Introducción al método Stepwise (paso a paso)

El método Stepwise es un grupo de algoritmos que tienen como objetivo automatizar la selección de variables en un modelo.

Se dividen principalmente en: Selección hacia atrás (backward) y hacia adelante (forward)

* Eliminación hacia atrás (backward)

Este es el más simple de todos los procedimientos de selección de variables. De hecho, se puede implementar fácilmente. Pasos:

1. Construir un modelo con todas las variables.
2. Eliminar una de las variable (según un criterio determinado). 
3. Volver a construir el modelo y retorna al paso 2.
4. Deternerse cuando la solución converja.

* Selección hacia adelante (forward)

Es el método anterior invertido.

1. Comenzar sin variables en el modelo.
2. Aplicar un criterio específico.
3. Agrega una variable usando este criterio.
4. Volver a construir el modelo y retorna al paso 2.
5. Continuar hasta que no se puedan agregar nuevas variables.

Aplicaremos ambos métodos al ejemplo del peso. Para eso, construiremos dos modelos: 

* fit1: el modelo con todas las variables.
* fit2: el modelo conteniendo, solamente, el Intercept.

```{r}
fit1 <- lm(Mass ~ .,data=mydata)
fit2 <- lm(Mass ~ 1,data=mydata)
```

***Nota: Los métodos backward y forward no necesariamente convergen el mismo subconjunto de variables.***

### Ejemplo utilizando backward

```{r}
library(MASS)
#It is a local optimum.
modelAIC1<-stepAIC(fit1,direction="backward")
```

El método finaliza, mostrando el listado de variables seleccionadas, esto es `Mass ~ Fore + Waist + Height + Calf + Thigh + Head`. 

### Ejemplo utilizando forward

```{r}
#It is a local optimum.
modelAIC2<-stepAIC(fit2,direction="forward",scope=list(upper=fit1,lower=fit2))
```

El método finaliza, mostrando el listado de variables seleccionadas. En este caso, el resultado es el mismo que en el método anterior: `Mass ~ Fore + Waist + Height + Calf + Thigh + Head`. 

### Ejemplo utilizando `direction=both`

También es posible indicar que el método avance en ambas direcciones. Es decir: que confluya comenzando desde los dos extremos a un punto intermedio. 

```{r}
#It is a local optimum.
fit3 <- lm(Mass ~ .*.,data=mydata)
modelAIC3<-stepAIC(fit1,direction="both",scope=list(upper=fit3,lower=fit2))
```

> RESUMEN: La regresión lineal ordinaria predice el valor esperado de una cantidad desconocida dada (la variable de respuesta, una variable aleatoria) como una combinación lineal de un conjunto de valores observados (predictores). Esto implica que un cambio constante en un predictor conduce a un cambio constante en la variable de respuesta (es decir, un modelo de respuesta lineal). Esto es apropiado cuando la variable de respuesta tiene una distribución normal.

---

# Regresión logística

Las regresiones lineas son muy útiles y eficientes siempre y cuando las predicciones (los datos de respuesta) que deseamos seamos continuas. En el ejemplo anterior, el peso (`Mass`) es una variable continua. De hecho, los valores predichos pertenecen a una recta (función continua).

Las ***regresiones logísticas*** se utilizan cuando necesitamos predicciones categóricas, binarias. Por ejemplo: "SI/NO", "SANO/ENFERMO", "TIENE/NO TIENE".

Ejemplo: 

Utilizando un dataset de pacientes de EEUU (32968 pacientes y 36 variables de cada paciente), vamos a predecir la probabilidad de tener hipertensión utilizando, como variables independientes: edad (age), sexo (sex), tiempo de sueño (sleep) e índice de masa corporal (bmi). 

Pasos: 

1. Descargar el archivo `LogisticRegresion&StepAIC.RData` a tu directorio de trabajo

2. Cargar los datos 

``` {r}
load("./resources/LogisticRegresion&StepAIC.RData")
```

3. Ver el contenido de cada columna: 

```{r, echo=FALSE}
labs <- attributes(NH11)$labels
knitr::kable(labs)
```

4. Chequear la estructura de `hypev`

```{r}
  str(NH11$hypev) # check stucture of hypev
```

5. Chequear los niveles de `hypev`

```{r}
  levels(NH11$hypev) # check levels of hypev
```

6. Borrar los datos NA

```{r}
NH11<-NH11[which(!is.na(NH11$hypev)),]
```

7.  Construir la regresión logística

```{r}
  hyp.out <- glm(hypev~age_p+sex+sleep+bmi,
                data=NH11, family="binomial")
  summary(hyp.out)
  coef(summary(hyp.out))
```

8. Validar

Ahora que tenemos el modelo es importante saber si "predice bien". Dos propuestas: 

a. ***Propuesta 1:*** Aplicar el modelo a los 32968 pacientes y ver si la predicción de cada uno coincide con el campo `NH11$hypev`. El porcentaje de coincidencia podría ser un índice de calidad del modelo. 

<span style="color:blue"> Esta propuesta es, en realidad, engañosa ya que estamos probando el predictor en los mismos datos que utilizamos para ***entrenar*** el modelo. *Es hacer un poco de trampa*. De ahí que esta propuesta, en el ámbito de la ciencia de datos, no es muy aceptable. </span>

b. ***Propuesta 2:*** Antes de construir el modelo (o sea, entre el paso 6 y el 7), se divide el dataset en dos: `train` y `test` (se puede hacer un split 70-30, por ejemplo). La construcción del modelo (paso 7) se realiza con los datos de train y la validación se realiza con los datos de `test`.

<span style="color:blue"> Esta propuesta es la que se utiliza de modo habitual.</span>

Para nuestro ejemplo, vamos a aplicar la ***propuesta a.*** (o sea: aplicaremos el modelo a los 32968 pacientes que usamos para el entrenamiento).  Predicción de la hipertensión de todos los pacientes: 

```{r}
predsAll<-predict(hyp.out, type = "response")

boxplot(predsAll ~ NH11$hypev, col = c("green", "red"),
        ylab = "Probabilidad",
        xlab = "Tiene / No tiene hypertensión")

plot(density(predsAll[which(NH11$hypev=="1 Yes")]), col ="dark green", main = "Density functions", ylim=c(0, 5) )
lines(density(predsAll[which(NH11$hypev=="2 No")]), col ="red", main = "Density functions")

```

9. Hacer predicciones

Por ejemplo, podemos preguntar "¿Cuánto más probable es que una mujer de 63 años tenga hipertensión en comparación con una mujer de 33 años?".

```{r}
  # Create a dataset with predictors set at desired levels
  predDat <- with(NH11,
                  expand.grid(age_p = c(33, 63),
                              sex = "2 Female",
                              bmi = mean(bmi, na.rm = TRUE),
                              sleep = mean(sleep, na.rm = TRUE)))
  # predict hypertension at those levels

preds <- predict(hyp.out, type = "response",
                         se.fit = TRUE, interval="confidence",
                         newdata = predDat)
  cbind(predDat, preds)
```

El resultado indica que una mujer de 33 años tiene un 13% de probabilidad de haber sido diagnosticada con hipertensión, mientras que una mujer de 63 años tiene un 48%. 


---

# Modelo lineal generalizado

John Nelder y Robert Wedderburn formularon modelos lineales generalizados como una forma de unificar otros modelos estadísticos, como la regresión lineal, la regresión logística y la regresión de Poisson. 

Los modelos lineales generalizados cubren todas estas situaciones al permitir variables de respuesta que tienen distribuciones arbitrarias (en lugar de simplemente distribuciones normales), y que una función arbitraria de la variable de respuesta (la función de enlace) varíe linealmente con los valores predichos. 

---

# Particionamiento Recursivo

Vamos a realizar una predicción construyendo un modelo de clasificación o regresión, del [paquete rpart](http://127.0.0.1:13413/help/library/rpart/doc/longintro.pdf). El modelo resultante se puede representar como un árbol binario.

Estos árboles de decisión construyen el modelo con técnicas de machine learning. Seleccionan la variable que mejor divide en dos a los datos, y la utiliza como raíz del árbol de decisión. Luego repite esa misma acción en cada una de las ramas hasta un cierto corte. 

En esta primera instancia conviene saber que `rpart` admite dos criterios de división de los datos: a) el índice de Gini y b) el índice de información. A los efectos de este curso, usaremos el índice de Gini. 

Para entender el funcionamiento, veremos un ejemplo. 

Vamos a utilizar los datos un dataset de R (llamado `cu.summary`) que contiene el grado de confiabilidad de 117 autos. 

```{r}
library(rpart)
str(cu.summary)
```

Las variables son: 

|Variable    | Descripción                                                    |
|:-----------|:---------------------------------------------------------------|
|Reliability | variable tipo factor (contiene NAs)                               |
|            | Much worse < worse < average < better < Much Better            |
|Price       | numérico: precio                                               |
|Country     |  pais donde fue fabricado                        |
|Mileage     | tamaño del tanque. Contiene NAs                      |
|Type        | Tipo: Small, Sporty, Compact, Medium, Large, Van             |

Vamos a predecir la ***confiabilidad*** (Reliability).

1. Veamos cómo son los datos de la variable de salida: 

```{r}
table(cu.summary$Reliability)
```

Hay 32 autos que no tienen indicado un nivel de confiabilidad: 

```{r}
table(is.na(cu.summary$Reliability))
```

2. Consutrcción del modelo (usando el índice de Gini) 

```{r}
fit1 <- rpart(Reliability ~ Price + Country + Mileage + Type, data = cu.summary, parms = list(split = 'gini'))
```

3. Visualización del árbol correspondiente (usando el paquete ´rattle´):

```{r echo=FALSE}
if (!is.element("rattle", installed.packages()[,1])){
  install.packages("rattle", repos = "http://mirror.fcaglp.unlp.edu.ar/CRAN/")
}
```

```{r}
library(rattle)
fancyRpartPlot(fit1)
```

Cómo leer el arbol? Vamos a interpretar los números del nodo raíz (nodo 1): 

- Hay un 21 % de "Much worse", 14 % de "worse", 31 % de "average", 9 % de "better" y 25 % de "Much Better" de autos, antes de hacer cualquier split. 
- "average" significa que el arbol eligió votar "en promedio".
- 100% significa que los porcentajes indicados corresponden a totalidad de la muestra. 

- Luego, si el auto fue fabricado en Brazil o Inglaterra, o Francia o Japón (nodo 3), hay un 11 % que sea "average", 11 % que sea "better" y un 78 % de que sea "Much Better". 
- El rótulo "Much better" del nodo 3 indica que, los autos que caigan en esta hoja del árbol serán calificados de "Much better".

- Los valores de los rótulos de las hojas indican la votación que el modelo hace de los casos que caigan en esa hoja. 

4. Para mayor conocimiento de los resultados: 

```{r}
summary(fit1)
```

5. Ahora que tenemos nuestro modelo, vamos a predecir la confiabilidad de los 32 autos que tienen NA en dicha columna: 

```{r}
# nos quedamos con los autos sin calificar
AutosSinCalificar<-cu.summary[is.na(cu.summary$Reliability),]

# realizamos las predicciones
Predictions <- predict(fit1, AutosSinCalificar, type = "class")

# copiamos las predicciones en la columna del data.frame
AutosSinCalificar$Reliability<-Predictions
knitr::kable(AutosSinCalificar)
```

6. ¿Cómo sabemos si las predicciones son buenas? Vamos a aplicar el predictor a los 85 autos y comparar el valor predicho con el valor indicado en el data.frame:

```{r}
allcars<-cu.summary
allcars<-allcars[!is.na(allcars$Reliability),]

predsAll <- predict(fit1, allcars, type = "class")

# Restamos predicho - indicado y lo sumarizamos
table(abs(as.numeric(predsAll)-as.numeric(allcars$Reliability)))

hist(abs(as.numeric(predsAll)-as.numeric(allcars$Reliability)), 
     main = "", xlab = "predsAll-allcars$Reliability", ylab = "cantidad de autos", 
     col="lightcyan", shadow=TRUE)
```

De los 85 autos, el arbol de regresión ha predicho:

- correctamente el 63.5 % de los autos (54/85)
- se ha equivocado por 1 categoría en el 18.82 % de los autos (16/85)
- se ha equivocado por 2 categorías en el 17.64 % de los autos (15/85)

Tarea: pruebe la performance del predictor cambiando el criterio de split (no utilizar 'gini' y utilizar 'information')

